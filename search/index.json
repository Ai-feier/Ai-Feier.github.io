[{"content":"生成 key $ openssl genrsa --out mkt.key 2048 根据 key 生成 csr CN: 为 mkt\n$ openssl req -new -key mkt.key -out mkt.csr -subj \u0026#34;/CN=mkt\u0026#34; 把 csr 发给 apiserver 的 ca 生成 crt $ openssl x509 -req -in mkt.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out mkt.crt -days 1095 Signature ok subject=CN = mkt Getting CA Private Key 生成文件:\nroot@master1:~/tmp# ls mkt.crt mkt.csr mkt.key 查看证书\n$ openssl x509 -in mkt.crt -text -noout Certificate: Data: Version: 1 (0x0) Serial Number: 01:60:fb:9a:ce:5e:59:28:b0:e3:d6:76:90:99:eb:52:41:a5:b9:86 Signature Algorithm: sha256WithRSAEncryption Issuer: CN = kubernetes Validity Not Before: Jan 3 06:44:53 2024 GMT Not After : Jan 2 06:44:53 2027 GMT Subject: CN = mkt Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: 00:d2:52:66:f1:f9:66:b6:9d:26:12:0e:1b:87:5d: 38:bb:56:a3:b0:ce:e1:49:91:b5:f5:cb:35:28:93: 1f:c8:55:7c:db:21:fc:84:ba:e9:15:27:e6:f9:fb: 38:19:0c:73:7a:0b:71:85:d9:66:f4:e4:5e:1c:3b: 6f:ea:b4:2b:e7:42:45:b2:96:fb:b9:74:97:f0:58: e7:ec:dd:04:54:05:81:37:45:e8:e1:13:d5:01:2e: 7e:34:48:63:63:56:90:b1:83:a7:79:c7:76:ee:03: 9c:1a:f6:e0:18:86:7b:12:54:c6:0f:fc:d3:63:4e: 62:f3:bc:ad:4a:c7:5e:a0:73:88:1e:df:46:72:c8: e2:84:11:5c:07:0c:23:58:81:f5:6d:15:9e:1c:48: fa:f5:76:1a:2b:0f:56:90:76:4f:06:3a:74:af:15: 87:23:c1:cf:04:69:fd:a1:91:d2:53:64:f8:02:da: 58:59:f2:ce:13:b2:40:91:da:fe:4d:2f:24:bf:fe: 6a:b7:ff:01:d8:4b:04:02:ab:f2:d6:e6:c2:61:af: 12:1e:53:ad:1a:cc:07:ee:f5:f1:d1:84:ef:67:01: ba:80:cf:21:61:87:bc:bb:d9:e6:25:de:b4:d7:23: 76:67:bb:b0:db:89:d0:53:c6:13:fa:31:30:32:5e: ba:f3 Exponent: 65537 (0x10001) Signature Algorithm: sha256WithRSAEncryption 86:3a:a2:70:d1:10:88:4f:b7:16:2b:76:37:52:2b:7d:c0:0b: 34:f9:fa:d3:ca:19:be:e8:20:5a:d6:e8:dd:19:46:4b:85:dd: b2:aa:74:3a:61:b1:96:a6:1c:8e:3c:fc:1f:5f:17:28:6e:0e: cc:be:e8:f9:f9:2f:02:cb:47:89:34:a3:9b:6b:d2:e6:3e:a4: e3:99:c4:cd:f9:2b:fe:bc:79:e1:d2:02:84:a3:e0:6c:90:e4: c9:76:1e:d8:52:56:96:61:f6:83:8d:f5:41:6f:50:49:ab:08: 24:32:e5:b1:1c:16:88:39:2e:a9:38:93:cd:32:df:f8:dc:c2: 32:c1:3d:14:fd:cf:ac:42:74:53:47:a9:e1:20:fc:88:3a:e3: 87:c7:b0:49:b2:46:11:0a:9f:1a:f3:d6:c4:1e:2d:7c:68:75: 87:08:43:ff:95:20:46:f3:8a:61:cc:54:72:bf:81:d8:2b:92: f1:0d:f8:ae:2e:b9:16:f1:f0:b3:a7:8e:0a:93:c4:0b:a1:c4: c3:bd:58:a0:e2:e1:f8:96:40:12:cd:de:97:54:a3:14:4c:fe: 37:61:e2:83:1b:50:2e:f9:a3:96:3e:6d:d2:09:67:56:63:07: 98:8c:14:33:69:e1:24:66:d2:20:33:2e:30:8f:92:a9:61:02: 7a:1a:97:49 查看 kubeconfig $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://11.0.1.150:6443 name: kubernetes contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: DATA+OMITTED client-key-data: DATA+OMITTED 创建 k8s 用户 $ kubectl config set-credentials mkt --client-certificate=mkt.crt --client-key=mkt.key --embed-certs=true User \u0026#34;mkt\u0026#34; set. 查看 kubeconfig\n$ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://11.0.1.150:6443 name: kubernetes contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: DATA+OMITTED client-key-data: DATA+OMITTED - name: mkt user: client-certificate-data: DATA+OMITTED client-key-data: DATA+OMITTED 查看 pod\n$ k get po -A --user mkt Error from server (Forbidden): pods is forbidden: User \u0026#34;mkt\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope 新建一个 context\n$ k config set-context mkt@kubernetes --cluster kubernetes --user mkt 切换 context\n$ k config use-context mkt@kubernetes $ k get po -A # 默认用户已为 mkt Error from server (Forbidden): pods is forbidden: User \u0026#34;mkt\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope 其中 \u0026ldquo;mkt\u0026rdquo; 是 csr 中的 CN\n为用户绑定 role\n# 创建 2 个 user $ kubectl config set-credentials user1 --client-certificate mkt.crt --client-key mkt.key --embed-certs=true $ kubectl config set-credentials user2 --client-certificate mkt.crt --client-key mkt.key --embed-certs=true # 创建 clusterrole $ kubectl create clusterrole podsOwe --verb=\u0026#39;*\u0026#39; --resource=pods # 创建 clusterrolebinding $ kubectl create clusterrolebinding pods-binding --clusterrole=podsOwe --user=mkt 查看 pod\n$ kubectl get po -A --user=user1 $ kubectl get po -A --user=user2 calico-apiserver calico-apiserver-69fc754d76-9w7p6 1/1 Running 4 (103m ago) 3d23h calico-apiserver calico-apiserver-69fc754d76-kdxt4 1/1 Running 4 (103m ago) 3d23h calico-system calico-kube-controllers-b9dcc57c4-gzlhk 1/1 Running 4 (103m ago) 3d23h calico-system calico-node-5jckf 1/1 Running 4 (103m ago) 3d23h calico-system calico-node-gq882 1/1 Running 4 (103m ago) 3d23h calico-system calico-node-zksvz 1/1 Running 4 (101m ago) 3d22h calico-system calico-typha-5cdb9d5d59-tnjzh 1/1 Running 4 (103m ago) 3d23h calico-system calico-typha-5cdb9d5d59-z7tnk 1/1 Running 4 (101m ago) 3d22h calico-system csi-node-driver-j4648 2/2 Running 8 (103m ago) 3d23h calico-system csi-node-driver-jf548 2/2 Running 8 (101m ago) 3d22h ...... 我们是把 rolebinding 绑给的 mkt, 但user1, user2均能获取 pod\n说明: 关键的是 csr 的 \u0026ldquo;CN\u0026rdquo;\n补充: 如果有分组的话, 就看 csr 的 \u0026ldquo;O\u0026rdquo;\n","date":"2024-01-03T17:22:24+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/ApiServer%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86/cover.png","permalink":"/p/kubernetes-%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86-apiserver-%E6%96%B0%E5%A2%9E%E9%9B%86%E7%BE%A4%E7%94%A8%E6%88%B7/","title":"Kubernetes 用户管理, ApiServer 新增集群用户"},{"content":"自动补全\necho \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.profile echo \u0026#34;alias k=kubectl\u0026#34; \u0026gt;\u0026gt; ~/.profile echo \u0026#34;complete -F __start_kubectl k\u0026#34; \u0026gt;\u0026gt; ~/.profile source ~/.profile vim 设置为粘贴模式, 防止缩进问题\necho \u0026#34;set paste\u0026#34; \u0026gt;\u0026gt; ~/.vimrc 1、权限控制 RBAC 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Context 为部署流水线创建一个新的 ClusterRole 并将其绑定到范围为特定的 namespace 的特定 ServiceAccount。 Task 创建一个名为 deployment-clusterrole 且仅允许创建以下资源类型的新 ClusterRole： Deployment StatefulSet DaemonSet 在现有的 namespace app-team1 中创建一个名为 cicd-token 的新 ServiceAccount。 限于 namespace app-team1 中，将新的 ClusterRole deployment-clusterrole 绑定到新的 ServiceAccount cicd-token。 考点： RBAC 授权模型的理解。\n参考链接: 没必要参考网址，使用-h 帮助更方便。\nkubectl create clusterrole -h\nkubectl create rolebinding -h\nhttps://kubernetes.io/docs/reference/access-authn-authz/rbac/#command-line-utilities\n解答: 更换 context $ kubectl config use-context k8s 创建 ClusterRole $ kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployment,daemonset,statefulset 创建 ServiceAccount $ kubectl create sa cicd-token -n app-team1 创建 rolebinding rolebinding 后面的名字 cicd-token-rolebinding 随便起的，因为题目中没有要求，如果题目中有要求，就不能随便起了。\nkubectl -n app-team1 create rolebinding cicd-token-rolebinding --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token 检查 $ kubectl -n app-team1 describe rolebinding cicd-token-rolebinding $ kubectl auth can-i create deployment --as system:serviceaccount:app-team1:cicd-token no $ kubectl auth can-i create deployment -n app-team1 --as system:serviceaccount:app-team1:cicd-token yes 2、查看 pod 的 CPU 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 通过 pod label name=cpu-loader，找到运行时占用大量 CPU 的 pod， 并将占用 CPU 最高的 pod 名称写入文件 /opt/KUTR000401/KUTR00401.txt（已存在）。 考点： kubectl top \u0026ndash;l 命令的使用\n参考链接: https://kubernetes.io/docs/reference/kubectl/cheatsheet/#interacting-with-running-pods\n解答: 更换 context $ kubectl config use-context k8s # 查看 pod 名称 -A 是所有 namespace $ kubectl top pod -l name=cpu-loader --sort-by=cpu -A # 将 cpu 占用最多的 pod 的 name 写入/opt/test1.txt 文件 $ echo \u0026#34;查出来的 Pod Name\u0026#34; \u0026gt; /opt/KUTR000401/KUTR00401.txt 检查 $ cat /opt/KUTR000401/KUTR00401.txt 3、配置网络策略 NetworkPolicy 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context hk8s Task 在现有的 namespace my-app 中创建一个名为 allow-port-from-namespace 的新 NetworkPolicy。 确保新的 NetworkPolicy 允许 namespace echo 中的 Pods 连接到 namespace my-app 中的 Pods 的 9000 端口。 进一步确保新的 NetworkPolicy： 不允许对没有在监听 端口 9000 的 Pods 的访问 不允许非来自 namespace echo 中的 Pods 的访问 双重否定就是肯定，所以最后两句话的意思就是：\n仅允许端口为 9000 的 pod 方法。\n仅允许 echo 命名空间中的 pod 访问。\n考点： NetworkPolicy 的创建\n参考链接: 依次点击 Concepts → Services, Load Balancing, and Networking → Network Policies（看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/concepts/services-networking/network-policies/\n解答: 更换 context $ kubectl config use-context hk8s 查看 namespace echo 的标签 # 查看所有 ns 的标签 label $ kubectl get ns --show-labels # 如果访问者的 namespace 没有标签 label，则需要手动打一个。如果有一个独特的标签 label，则也可以直接使用。 $ kubectl label ns echo project=echo 创建 networkpolicy vim networkpolicy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-port-from-namespace namespace: my-app #被访问者的命名空间 spec: podSelector: #这两行必须要写，或者也可以写成一行为 podSelector: {} matchLabels: {} # 注意 matchLabels:与{}之间有一个空格 policyTypes: - Ingress #策略影响入栈流量 ingress: - from: #允许流量的来源 - namespaceSelector: matchLabels: project: echo #访问者的命名空间的标签 label #- podSelector: {} #注意，这个不写。如果 ingress 里也写了- podSelector: {}，则会导致 my-app 中的 pod 可以访问 my-app 中 pod 的 9000 了，这样不 满足题目要求不允许非来自 namespace echo 中的 Pods 的访问。 ports: - protocol: TCP port: 9000 #被访问者公开的端口 创建 $ kubectl apply -f networkpolicy.yaml 检查 $ kubectl describe networkpolicy -n my-app 4、暴露服务 service 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 请重新配置现有的 deployment front-end 以及添加名为 http 的端口规范来公开现有容器 nginx 的端口 80/tcp。 创建一个名为 front-end-svc 的新 service，以公开容器端口 http。 配置此 service，以通过各个 Pod 所在的节点上的 NodePort 来公开他们。 考点： 将现有的 deploy 暴露成 nodeport 的 service。\n参考链接: 依次点击 Concepts → Workloads → Workload Resources → Deployments（看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/\n解答: 更换 context $ kubectl config use-context k8s 修改 deployment $ kubectl edit deployment front-end ...... spec: containers: - image: vicuu/nginx:hello imagePullPolicy: IfNotPresent name: nginx #找到此位置。下文会简单说明一下 yaml 文件的格式，不懂 yaml 格式的，往下看。 ports: #添加这 4 行 - name: http containerPort: 80 protocol: TCP ...... 暴露端口 $ kubectl expose deployment front-end --type=NodePort --port=80 --target-port=80 --name=front-end-svc 检查 $ kubectl get svc front-end-svc -o wide 5、创建 Ingress 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 如下创建一个新的 nginx Ingress 资源： 名称: ping Namespace: ing-internal 使用服务端口 5678 在路径 /hello 上公开服务 hello 可以使用以下命令检查服务 hello 的可用性，该命令应返回 hello： curl -kL \u0026lt;INTERNAL_IP\u0026gt;/hello 考点： Ingress 的创建\n参考链接: 依次点击 Concepts → Services, Load Balancing, and Networking → Ingress （看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/\n解答: 更换 context $ kubectl config use-context k8s 创建 ingressclass $ vim ingressclass.yaml apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller name: nginx annotations: ingressclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; spec: controller: k8s.io/ingress-nginx $ kubectl apply -f ingressclass.yaml 创建 ingress $ vim ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ping namespace: ing-internal annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - http: paths: - path: /hello pathType: Prefix backend: service: name: hello port: number: 5678 $ kubectl apply -f ingress.yaml 检查 $ kubectl apply -f ingress.yaml 也可以 curl 一下 ingress 地址 6、扩容 deployment 副本数量 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 将 deployment presentation 扩展至 4 个 pods 考点： deployment 扩缩容\n参考链接: 没必要参考网址，使用-h 帮助更方便。\nkubectl scale deployment -h\nhttps://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment https://kubernetes.io/zh-cn/docs/tasks/run-application/scale-stateful-set/\n解答: 更换 context $ kubectl config use-context k8s 修改replicaset $ kubectl scale deployment presentation --replicas=4 检查 $ kubectl get deployment presentation -oyaml 7、调度 pod 到指定节点 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 按如下要求调度一个 pod： 名称：nginx-kusc00401 Image：nginx Node selector：disk=ssd 考点： nodeSelect 属性的使用\n参考链接: 依次点击 Tasks → Configure Pods and Containers → Assign Pods to Nodes （看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/\n解答: 更换 context $ kubectl config use-context k8s 创建 pod yaml # 获取一个 pod 模版 $ kubectl run nginx-kusc00401 --image=nginx --dry-run -oyaml \u0026gt; 7.pod.yaml $ cat 7.pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx-kusc00401 spec: nodeSelector: # 新增, 其余内容不需要 disk: ssd containers: - image: nginx name: nginx-kusc00401 创建 pod $ kubectl create -f 7.pod.yaml pod/nginx-kusc00401 created 检查 $ kubectl get po -owide 8、查看可用节点数量 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 检查有多少 nodes 已准备就绪（不包括被打上 Taint：NoSchedule 的节点）， 并将数量写入 /opt/KUSC00402/kusc00402.txt 考点： 检查节点角色标签，状态属性，污点属性的使用\n参考链接: 没必要参考网址，使用-h 帮助更方便。\nkubectl -h\nhttps://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/\n解答: 更换 context $ kubectl config use-context k8s 检查就绪 node $ k describe no | grep -i taint Taints: node-role.kubernetes.io/control-plane:NoSchedule Taints: node-role.kubernetes.io/control-plane:NoSchedule Taints: \u0026lt;none\u0026gt; # 考试根据实际环境, 这里有 2 个节点被打上 NoSchedule $ echo 1 \u0026gt; /opt/KUSC00402/kusc00402.txt 检查 $ cat /opt/KUSC00402/kusc00402.txt 9、创建多容器的 pod 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 按如下要求调度一个 Pod： 名称：kucc8 app containers: 2 container 名称/images： ⚫ nginx ⚫ consul 考点： pod 概念\n参考链接: 依次点击 Concepts → Workloads → Pods （看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/concepts/workloads/pods/\n解答: 更换 context $ kubectl config use-context k8s 创建 pod yaml 没指定 namespace 就是 default 不用填\n# 获取一个 pod 模版 $ kubectl run kucc8 --image=nginx --dry-run=client -oyaml \u0026gt; 9.pod.yaml $ cat 9.pod.yaml apiVersion: v1 kind: Pod metadata: name: kucc8 spec: containers: - image: nginx name: nginx - image: cousul name: consul 创建 pod $ kubectl create -f 9.pod.yaml 检查 $ kubectl get pod 10、创建 PV 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context hk8s Task 创建名为 app-config 的 persistent volume，容量为 1Gi，访问模式为 ReadWriteMany。 volume 类型为 hostPath，位于 /srv/app-config 考点： hostPath 类型的 pv\n参考链接: 依次点击 Tasks → Configure Pods and Containers → Configure a Pod to Use a PersistentVolume for Storage （看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\n解答: 更换 context $ kubectl config use-context hk8s 创建 pv yaml # 复制官网模版, 修改 $ cat 10.pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: app-config # 名 labels: type: local spec: # storageClassName: manual 题目没指定 capacity: storage: 1Gi # 容量 accessModes: - ReadWriteMany # 读写权限 hostPath: path: \u0026#34;/srv/app-config\u0026#34; # 题目有指定 创建 pv $ kubectl create -f 10.pv.yaml 检查 $ kubectl describe pv app-config 11、创建 PVC 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context ok8s Task 创建一个新的 PersistentVolumeClaim： 名称: pv-volume Class: csi-hostpath-sc 容量: 10Mi 创建一个新的 Pod，来将 PersistentVolumeClaim 作为 volume 进行挂载： 名称：web-server Image：nginx:1.16 挂载路径：/usr/share/nginx/html 配置新的 Pod，以对 volume 具有 ReadWriteOnce 权限。 最后，使用 kubectl edit 或 kubectl patch 将 PersistentVolumeClaim 的容量扩展为 70Mi，并记录此更改。 考点： pvc 的创建 class 属性的使用，\u0026ndash;record 记录变更\n参考链接: 依次点击 Tasks → Configure Pods and Containers → Configure a Pod to Use a PersistentVolume for Storage （看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\n解答: 更换 context $ kubectl config use-context ok8s 创建 pvc # 复制官网模版, 按题目修改响应字段 $ cat 11.pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pv-volume spec: storageClassName: csi-hostpath-sc accessModes: - ReadWriteOnce resources: requests: storage: 10Mi $ kubectl create -f 11.pvc.yaml 创建 pod # 获取 pod 模版 $ kubectl run web-server --image=nginx:1.16 --dry-run=client -oyaml \u0026gt; 11.pod.yaml $ cat 11.pod.yaml # 挂载 pvc apiVersion: v1 kind: Pod metadata: labels: run: web-server name: web-server spec: volumes: - name: 11-pvc persistentVolumeClaim: claimName: pv-volume containers: - image: nginx:1.16 name: web-server volumeMounts: - mountPath: \u0026#34;/usr/share/nginx/html\u0026#34; name: 11-pvc $ kubectl create -f 11.pod.yaml 修改 pvc 并记录 $ kubectl edit pvc pv-volume --record 12、查看 pod 日志 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Task 监控 pod foo 的日志并： 提取与错误 RLIMIT_NOFILE 相对应的日志行 将这些日志行写入 /opt/KUTR00101/foo 考点： kubectl logs 命令\n参考链接: 没必要参考网址，使用-h 帮助更方便。\nkubectl -h\nhttps://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#examine-pod-logs\n解答: $ kubectl logs foo | grep RLIMIT_NOFILE \u0026gt; /opt/KUTR00101/foo 检查 $ cat /opt/KUTR00101/foo 13、使用 sidecar 代理容器日志 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context k8s Context 将一个现有的 Pod 集成到 Kubernetes 的内置日志记录体系结构中（例如 kubectl logs）。 添加 streaming sidecar 容器是实现此要求的一种好方法。 Task 使用 busybox Image 来将名为 sidecar 的 sidecar 容器添加到现有的 Pod 11-factor-app 中。 新的 sidecar 容器必须运行以下命令： /bin/sh -c tail -n+1 -f /var/log/11-factor-app.log 使用挂载在/var/log 的 Volume，使日志文件 11-factor-app.log 可用于 sidecar 容器。 除了添加所需要的 volume mount 以外，请勿更改现有容器的规格。 考题翻译成白话，就是：\n添加一个名为 sidecar 的边车容器(使用 busybox 镜像)，加到已有的 pod 11-factor-app 中。 确保 sidecar 容器能够输出 /var/log/11-factor-app.log 的信息。\n使用 volume 挂载 /var/log 目录，确保 sidecar 能访问 11-factor-app.log 文件\n考点： pod 两个容器共享存储卷\n参考链接: （需要复制网页内容）\n依次点击 Concepts → Cluster Administration → Logging Architecture （看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/concepts/cluster-administration/logging/\n解答: 更换 context $ kubectl config use-context k8s 获取 pod yaml $ kubectl get pod 11-factor-app -o yaml \u0026gt; varlog.yaml # 备份 yaml 文件，防止改错了，回退。 $ cp varlog.yaml varlog-bak.yaml 修改 yaml spec: 。。。。。。 volumeMounts: #在原配置文件，灰色的这段后面添加 - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-4l6w8 readOnly: true - name: varlog #新加内容 mountPath: /var/log #新加内容 - name: sidecar #新加内容，注意 name 别写错了 image: busybox #新加内容 args: [/bin/sh, -c, \u0026#39;tail -n+1 -f /var/log/11-factor-app.log\u0026#39;] #新加内容，注意 文件名 别写错了。另外是用逗号分隔的，而题目里是空格。 volumeMounts: #新加内容 - name: varlog #新加内容 mountPath: /var/log #新加内容 dnsPolicy: ClusterFirst enableServiceLinks: true 。。。。。。 volumes: #在原配置文件，灰色的这段后面添加。 - name: kube-api-access-kcjc2 projected: defaultMode: 420 sources: - serviceAccountToken: expirationSeconds: 3607 path: token - configMap: items: - key: ca.crt path: ca.crt name: kube-root-ca.crt - downwardAPI: items: - fieldRef: apiVersion: v1 fieldPath: metadata.namespace path: namespace - name: varlog #新加内容，注意找好位置。 emptyDir: {} #新加内容 创建 pod $ kubectl delete -f varlog.yaml $ kubectl createe -f varlog.yaml 检查 $ kubectl exec 11-factor-app -c sidecar -- tail -f /var/log/11-factor-app.log $ kubectl exec 11-factor-app -c count -- tail -f /var/log/11-factor-app.log 14、升级集群 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context mk8s Task 现有的 Kubernetes 集群正在运行版本 1.28.0。仅将 master 节点上的所有 Kubernetes 控制平面和节点组件升级到版本 1.28.1。 确保在升级之前 drain master 节点，并在升级后 uncordon master 节点。 可以使用以下命令，通过 ssh 连接到 master 节点： ssh master01 可以使用以下命令，在该 master 节点上获取更高权限： sudo -i 另外，在主节点上升级 kubelet 和 kubectl。 请不要升级工作节点，etcd，container 管理器，CNI 插件， DNS 服务或任何其他插件。 考点： 如何离线主机，并升级控制面板和升级节点\n参考链接: 没必要参考网址，建议多练习，背过命令就行。\n记不清的，可以使用 kubectl -h 来帮助。\n如果非要参考，可以按照下面方法。\n依次点击 Tasks → Administer a Cluster → Administration with kubeadm → Upgrading kubeadm clusters （看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/\n解答: 更换 context $ kubectl config use-context mk8s ssh 到 master01 $ ssh master01 $ sudo -i 排空节点 $ kubectl cordon master $ kubectl drain master 升级 kubeadm $ apt update $ apt-cache madison kubeadm | grep 1.28.1 # apt 升级 kubeadm $ apt-get install kubeadm=1.28.1-00 -y # kubeadm 升级集群, 注意不要升级 etcd, 忘记怎么写, 可以 -h $ kubeadm upgrade apply 1.28.1 --etcd-upgrade=false 升级 kubectl, kubelet $ apt-get install kubectl=1.28.1-00 kubelet=1.28.1-00 -y 检查 $ kubeadm version $ kubectl version $ kubelet --version # 记得使节点可重新调度 $ kubectl uncordon master01 $ exit # 退到 node01 15、备份还原 etcd 题目: 设置配置环境 此项目无需更改配置环境。但是，在执行此项目之前，请确保您已返回初始节点。 [candidate@master01] $ exit #注意，这个之前是在 master01 上，所以要 exit 退到 node01，如果已经是 node01 了，就不要再 exit 了。 Task 首先，为运行在 https://127.0.0.1:2379 上的现有 etcd 实例创建快照并将快照保存到 /var/lib/backup/etcd-snapshot.db 为给定实例创建快照预计能在几秒钟内完成。 如果该操作似乎挂起，则命令可能有问题。用 CTRL + C 来取消操作，然后重试。 然后还原位于/data/backup/etcd-snapshot-previous.db 的现有先前快照。 提供了以下 TLS 证书和密钥，以通过 etcdctl 连接到服务器。 CA 证书: /opt/KUIN00601/ca.crt 客户端证书: /opt/KUIN00601/etcd-client.crt 客户端密钥: /opt/KUIN00601/etcd-client.key 考点： etcd 的备份和还原命令\n参考链接: 没必要参考网址，建议多练习，背过命令就行。\n记不清的，可以使用 etcdctl -h 来帮助，更方便。\n如果非要参考，可以按照下面方法。\n依次点击 Tasks → Administer a Cluster → Operating etcd clusters for Kubernetes （看不懂英文的，可右上角翻译成中文）\nhttps://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/\n解答: 注意:\n记得设置 ETCDCTL_API=3 如果文件没权限, 就 sudo -i 备份快照 $ sudo ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/opt/KUIN00601/ca.crt --cert=/opt/KUIN00601/etcd-client.crt --key=/opt/KUIN00601/etcd-client.key snapshot save /var/lib/backup/etcd-snapshot.db 恢复快照 $ sudo ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/opt/KUIN00601/ca.crt --cert=/opt/KUIN00601/etcd-client.crt --key=/opt/KUIN00601/etcd-client.key snapshot restore /data/backup/etcd-snapshot-previous.db 检查 # 没必要 $ etcdctl snapshot status /var/lib/backup/etcd-snapshot.db -wtable 16、排查集群中故障节点 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context wk8s Task 名为 node02 的 Kubernetes worker node 处于 NotReady 状态。 调查发生这种情况的原因，并采取相应的措施将 node 恢复为 Ready 状态，确保所做的任何更改永久生效。 可以使用以下命令，通过 ssh 连接到 node02 节点： ssh node02 可以使用以下命令，在该节点上获取更高权限： sudo -i 参考链接: 没必要参考网址，记住先 restart 再 enable 就行。\nhttps://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/kubelet-integration/\n解答: 更换 context $ kubectl config use-context wk8s ssh 到 node02 查看 kubelet $ ssh node02 $ sudo -i # root # 查看 kubelet 状态 $ systemctl status kubelet $ systemctl restart kubelet \u0026amp;\u0026amp; systemctl enable kubelet 检查 $ systemctl status kubelet # 考试时记得从 node2 退回到 node1 $ exit $ exit 17、节点维护 题目: 设置配置环境： [candidate@node-1] $ kubectl config use-context ek8s Task 将名为 node02 的 node 设置为不可用，并重新调度该 node 上所有运行的 pods。 考点： cordon 和 drain 命令的使用\n参考链接: 没必要参考网址，使用-h 帮助更方便。\nkubectl -h\nhttps://kubernetes.io/zh-cn/docs/tasks/administer-cluster/safely-drain-node/\n解答: 更换 context $ kubectl config use-context ek8s 排空节点 $ kubectl cordon node02 $ kubectl drain node02 ","date":"2024-01-02T17:50:40+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/cka%20%26%20cks/cert.png","permalink":"/p/2024-cka-%E9%A2%98%E5%BA%93/","title":"2024 CKA 题库"},{"content":" 源码地址: https://github.com/Ai-feier/gracefulshutdown\n思考优雅终止目标 应用开始关闭时, 对于服务器的连接 对已有连接: 等待其处理 拒绝新请求 是否需要将 cache 中数据保存到 db 释放服务器资源 对象关系图 应用启动: 优雅终止: 优雅的优雅终止实现 需监听的信号量 windows var Signals = []os.Signal{ os.Interrupt, os.Kill, syscall.SIGKILL, syscall.SIGHUP, syscall.SIGINT, syscall.SIGQUIT, syscall.SIGILL, syscall.SIGTRAP, syscall.SIGABRT, syscall.SIGTERM, } linux var Signals = []os.Signal{ os.Interrupt, os.Kill, syscall.SIGKILL, syscall.SIGSTOP, syscall.SIGHUP, syscall.SIGINT, syscall.SIGQUIT, syscall.SIGILL, syscall.SIGTRAP, syscall.SIGABRT, syscall.SIGSYS, syscall.SIGTERM, } Server 一个 server 对应一个 service\n定义 // Server 对应一个服务 type Server struct { svc *http.Server name string mux *serverMux } // serverMux 请求锁 // 装饰器模式 type serverMux struct { reject bool *http.ServeMux } 其中使用 http.ServeMux 是为确保一个 server, 只能持有一个正在处理的 request\nnew 传入 server 名字, 监听地址\nfunc NewServer(name, addr string) *Server { mux := \u0026amp;serverMux{ServeMux: http.NewServeMux()} return \u0026amp;Server{ name: name, mux: mux, svc: \u0026amp;http.Server{ Handler: mux, Addr: addr, }, } } server 的启动与终止 func (s *Server) Start() error { return s.svc.ListenAndServe() } func (s *Server) Handle(pattern string, handler http.HandlerFunc) { s.mux.Handle(pattern, handler) } func (s *Server) reject() { s.mux.reject = true } func (s *Server) stop(ctx context.Context) error { log.Println(\u0026#34;server: \u0026#34;, s.name, \u0026#34;关闭中\u0026#34;) return s.svc.Shutdown(ctx) } App 定义 // 扩展 App (option 设计模式) type AppOption func(app *App) type App struct { servers []*Server // app 关闭的最长时间 shutdownTimeout time.Duration // 留给 server 处理已有请求的时间 waitTime time.Duration // 回调函数超时控制 cbTime time.Duration cbs []ShutdownCallBack } new // NewApp 新建 App // 需用户传入 server func NewApp(servers []*Server, opts ...AppOption) *App { app := \u0026amp;App{ servers: servers, shutdownTimeout: time.Second * 30, waitTime: time.Second * 10, cbTime: 3 * time.Second, } for _, opt := range opts { opt(app) } return app } // WithShutDownCallBack 向 App 传入 callback 函数 func WithShutDownCallBack(cbs ...ShutdownCallBack) AppOption { return func(app *App) { app.cbs = cbs } } app 启动与优雅终止 func (app *App) StartAndServer() { // 启动服务 for _, s := range app.servers { svc := s go func() { if err := svc.Start(); err != nil { if errors.Is(err, http.ErrServerClosed) { log.Printf(\u0026#34;服务器%s已关闭\u0026#34;, svc.name) } else { log.Printf(\u0026#34;服务器%s异常退出\u0026#34;, svc.name) } } }() } log.Println(\u0026#34;应用启动成功\u0026#34;) // app 启动成功 // 监听退出信号 // 监听两次信号, 第一次优雅终止, 第二次强行终止 ch := make(chan os.Signal, 2) signal.Notify(ch, sig.Signals...) \u0026lt;-ch fmt.Println(\u0026#34;应用开始关闭...\u0026#34;) go func() { select { case \u0026lt;-ch: log.Println(\u0026#34;强制退出\u0026#34;) os.Exit(1) case \u0026lt;-time.After(app.shutdownTimeout): log.Println(\u0026#34;超时强行退出\u0026#34;) os.Exit(1) } }() app.shutdown(context.Background()) } func (app *App) shutdown(ctx context.Context) { log.Println(\u0026#34;app start to shutdown\u0026#34;) // 将 app 下所有 server 拒绝新请求 for _, svc := range app.servers { svc.reject() } log.Println(\u0026#34;等待已有请求处理\u0026#34;) // 这里可以改造为实时统计正在处理的请求数量，为0 则下一步 time.Sleep(time.Second * app.waitTime) log.Println(\u0026#34;开始关闭应用\u0026#34;) var wg sync.WaitGroup wg.Add(len(app.servers)) for _, s := range app.servers { svc := s go func() { if err := svc.stop(ctx);err != nil { log.Println(\u0026#34;服务器关闭失败\u0026#34;, svc.name) } wg.Done() }() } wg.Wait() // 执行回调函数 wg.Add(len(app.cbs)) log.Println(\u0026#34;开始执行回调函数\u0026#34;) for _, cb := range app.cbs { c := cb go func() { ctx2, cancal := context.WithCancel(ctx) c(ctx2) cancal() wg.Done() }() } wg.Wait() // 释放资源 log.Println(\u0026#34;开始释放资源\u0026#34;) app.close() } CallBackFunc 执行时间: 在优雅终止期间 app 确保所有 server 已关闭后调用\n由用户自定义实现\n示例 func StoreCacheToDBCallBack(ctx context.Context) { done := make(chan struct{}, 1) go func() { // 这里将 cache 中数据刷到 db log.Println(\u0026#34;缓存刷新中...\u0026#34;) time.Sleep(time.Second) done \u0026lt;- struct{}{} }() select { case \u0026lt;-done: log.Printf(\u0026#34;缓存被刷新到了 DB\u0026#34;) case \u0026lt;-ctx.Done(): log.Printf(\u0026#34;刷新缓存超时\u0026#34;) } } 完整代码 main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;github.com/Ai-feier/http/server\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func main() { // 新建两个 server s1 := server.NewServer(\u0026#34;service1\u0026#34;, \u0026#34;localhost:8081\u0026#34;) s1.Handle(\u0026#34;/\u0026#34;, func(writer http.ResponseWriter, request *http.Request) { _, _ = writer.Write([]byte(\u0026#34;hello world\u0026#34;)) }) s2 := server.NewServer(\u0026#34;service2\u0026#34;, \u0026#34;localhost:8082\u0026#34;) // 新建应用, 包含 service1, service2 app := server.NewApp([]*server.Server{s1, s2}, server.WithShutDownCallBack(StoreCacheToDBCallBack)) app.StartAndServer() } func StoreCacheToDBCallBack(ctx context.Context) { done := make(chan struct{}, 1) go func() { // 这里将 cache 中数据刷到 db log.Println(\u0026#34;缓存刷新中...\u0026#34;) time.Sleep(time.Second) done \u0026lt;- struct{}{} }() select { case \u0026lt;-done: log.Printf(\u0026#34;缓存被刷新到了 DB\u0026#34;) case \u0026lt;-ctx.Done(): log.Printf(\u0026#34;刷新缓存超时\u0026#34;) } } server.go package server import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/Ai-feier/sig\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) type AppOption func(app *App) // ShutdownCallBack App 关闭时的回调函数 // 默认超时 3s // 用户可通过 context 自行控制 type ShutdownCallBack func(ctx context.Context) // WithShutDownCallBack 向 App 传入 callback 函数 func WithShutDownCallBack(cbs ...ShutdownCallBack) AppOption { return func(app *App) { app.cbs = cbs } } type App struct { servers []*Server // app 关闭的最长时间 shutdownTimeout time.Duration // 留给 server 处理已有请求的时间 waitTime time.Duration // 回调函数超时控制 cbTime time.Duration cbs []ShutdownCallBack } // NewApp 新建 App // 需用户传入 server func NewApp(servers []*Server, opts ...AppOption) *App { app := \u0026amp;App{ servers: servers, shutdownTimeout: time.Second * 30, waitTime: time.Second * 10, cbTime: 3 * time.Second, } for _, opt := range opts { opt(app) } return app } func (app *App) StartAndServer() { // 启动服务 for _, s := range app.servers { svc := s go func() { if err := svc.Start(); err != nil { if errors.Is(err, http.ErrServerClosed) { log.Printf(\u0026#34;服务器%s已关闭\u0026#34;, svc.name) } else { log.Printf(\u0026#34;服务器%s异常退出\u0026#34;, svc.name) } } }() } log.Println(\u0026#34;应用启动成功\u0026#34;) // app 启动成功 // 监听退出信号 // 监听两次信号, 第一次优雅终止, 第二次强行终止 ch := make(chan os.Signal, 2) signal.Notify(ch, sig.Signals...) \u0026lt;-ch fmt.Println(\u0026#34;应用开始关闭...\u0026#34;) go func() { select { case \u0026lt;-ch: log.Println(\u0026#34;强制退出\u0026#34;) os.Exit(1) case \u0026lt;-time.After(app.shutdownTimeout): log.Println(\u0026#34;超时强行退出\u0026#34;) os.Exit(1) } }() app.shutdown(context.Background()) } func (app *App) shutdown(ctx context.Context) { log.Println(\u0026#34;app start to shutdown\u0026#34;) // 将 app 下所有 server 拒绝新请求 for _, svc := range app.servers { svc.reject() } log.Println(\u0026#34;等待已有请求处理\u0026#34;) // 这里可以改造为实时统计正在处理的请求数量，为0 则下一步 time.Sleep(time.Second * app.waitTime) log.Println(\u0026#34;开始关闭应用\u0026#34;) var wg sync.WaitGroup wg.Add(len(app.servers)) for _, s := range app.servers { svc := s go func() { if err := svc.stop(ctx);err != nil { log.Println(\u0026#34;服务器关闭失败\u0026#34;, svc.name) } wg.Done() }() } wg.Wait() // 执行回调函数 wg.Add(len(app.cbs)) log.Println(\u0026#34;开始执行回调函数\u0026#34;) for _, cb := range app.cbs { c := cb go func() { ctx2, cancal := context.WithCancel(ctx) c(ctx2) cancal() wg.Done() }() } wg.Wait() // 释放资源 log.Println(\u0026#34;开始释放资源\u0026#34;) app.close() } func (app *App) close() { // 可补充释放资源逻辑 time.Sleep(time.Second) log.Println(\u0026#34;应用关闭\u0026#34;) } // Server 对应一个服务 type Server struct { svc *http.Server name string mux *serverMux } // serverMux 请求锁 //装饰器模式 type serverMux struct { reject bool *http.ServeMux } func (s *serverMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { if s.reject { w.WriteHeader(http.StatusInternalServerError) _, _ = w.Write([]byte(\u0026#34;服务器已关闭\u0026#34;)) return } s.ServeMux.ServeHTTP(w, r) } func NewServer(name, addr string) *Server { mux := \u0026amp;serverMux{ServeMux: http.NewServeMux()} return \u0026amp;Server{ name: name, mux: mux, svc: \u0026amp;http.Server{ Handler: mux, Addr: addr, }, } } func (s *Server) Start() error { return s.svc.ListenAndServe() } func (s *Server) Handle(pattern string, handler http.HandlerFunc) { s.mux.Handle(pattern, handler) } func (s *Server) reject() { s.mux.reject = true } func (s *Server) stop(ctx context.Context) error { log.Println(\u0026#34;server: \u0026#34;, s.name, \u0026#34;关闭中\u0026#34;) return s.svc.Shutdown(ctx) } 效果演示 C:\\Users\\26645\\Desktop\\goproject\\gracefulshutdown\\http\u0026gt;go build . C:\\Users\\26645\\Desktop\\goproject\\gracefulshutdown\\http\u0026gt;http.exe 2024/01/02 15:32:08 应用启动成功 应用开始关闭... 2024/01/02 15:32:10 app start to shutdown 2024/01/02 15:32:10 等待已有请求处理 2024/01/02 15:32:10 开始关闭应用 2024/01/02 15:32:10 server: service2 关闭中 2024/01/02 15:32:10 server: service1 关闭中 2024/01/02 15:32:10 服务器service2已关闭 2024/01/02 15:32:10 服务器service1已关闭 2024/01/02 15:32:10 开始执行回调函数 2024/01/02 15:32:10 缓存刷新中... 2024/01/02 15:32:11 缓存被刷新到了 DB 2024/01/02 15:32:11 开始释放资源 2024/01/02 15:32:12 应用关闭 ","date":"2024-01-02T16:57:31+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/%E4%BC%98%E9%9B%85%E7%BB%88%E6%AD%A2.http/image-20240102163454049.png","permalink":"/p/%E4%BC%98%E9%9B%85%E7%BB%88%E6%AD%A2-%E5%9F%BA%E4%BA%8E-go-%E7%9A%84-http-%E5%BA%93%E5%AE%9E%E7%8E%B0/","title":"优雅终止 | 基于 go 的 http 库实现"},{"content":"k8s 集群配置 IP Host 配置 11.0.1.150 master1 (keepalived+haproxy) 2C 4G 30G 11.0.1.148 master2 (keepalived+haproxy) 2C 4G 30G 11.0.1.149 node1 2C 4G 30G vip 地址: https://11.0.1.100:16443\n认识 kubeconfig kubeconfig 即为 kubectl 用于管理 k8s 集群的配置文件\n关系图解\ncurrent-context 选择 context 操作相应集群 context 将 user 与 cluster 联系起来 user 提供要操作集群的 cert 与 key cluster 提供集群的 ca 与 集群的入口地址 user 与 cluster 是弱关联\nkubectl config 参数详解 $ kubectl config --help Available Commands: current-context 显示当前上下文 delete-cluster 删除kubeconfig文件中指定的集群 delete-context 删除kubeconfig文件中指定的context delete-user 删除kubeconfig文件中指定的user get-clusters 显示kubeconfig文件中定义的集群 get-contexts 显示一个或多个contexts get-users 显示kubeconfig文件中定义的users rename-context 重命名kubeconfig文件里的context set 在kubecconfig文件中设置一个单独的值 set-cluster 在kubecconfig中设置集群条目 set-context 在kubecconfig中设置上下文条目 set-credentials 在kubecconfig中设置一个用户条目 unset 取消kubecconfig文件中单个值的设置 use-context 在kubecconfig文件中设置当前上下文(kubecconfig文件可以有多个上下文) view 显示合并的 kubeconfig 配置或一个指定的 kubeconfig 文件 kubeconifg 使用 kubectl 自动加载 ~/.kube/config 如果指定 $KUBECONFIG, 从$KUBECONFIG加载 也可以通过 kubectl 指定: kubectl config get-contexts \u0026ndash;kubeconfig=/root/.kube/config1 配置文件参考: apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBD...... server: https://11.0.1.100:16443 # 指定集群的入口 name: kubernetes contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes # 指定当前 context 上下文 kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: ...... client-key-data: ...... 生成新的 context # 获取 kubeconfig 中所有 cluster $ kubectl config get-clusters NAME kubernetes $ kubectl config set-context test --cluster=kubernetes --user=kubernetes-admin --kubeconfig=/root/.kube/config --namespace=test # --kubeconfig kubeconfig 是默认路径可省略 # --namespace 指定 context 的默认 namespace, 没有即为 default 新增 context: ...... contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes - context: # 新增 context cluster: kubernetes namespace: test user: kubernetes-admin name: test ... 切换 context 上下文 # 查看当前 context $ kubectl config current-context kubernetes-admin@kubernetes # 查看 kubeconfig 中所有 context $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE kubernetes-admin@kubernetes kubernetes kubernetes-admin * test kubernetes kubernetes-admin test # 切换 context $ kubectl config use-context test Switched to context \u0026#34;test\u0026#34;. # 创建 pod | 默认在 test namespace 下创建 $ k get po NAME READY STATUS RESTARTS AGE nginxpod 1/1 Running 0 14m 多集群管理 因集群配置有 vip, 本文将新增一个 cluster 直接指向 master apiserver 的地址, 所以, cluster 的 ca 不变\n$ cat ~/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBD...... server: https://11.0.1.100:16443 name: kubernetes - cluster: certificate-authority-data: LS0tLS1CRUdJTiBD...... server: https://11.0.1.150:6443 name: direct-cluster contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes - context: cluster: kubernetes namespace: test user: kubernetes-admin name: test current-context: test kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: ...... client-key-data: ...... 我们新增了一个direct-cluster的cluster\n创建 context # 默认 namespace 为 default $ kubectl config set-context direct --user=kubernetes-admin --cluster=direct-cluster # 切换 context $ kubectl config use-context direct $ kubectl get po No resources found in default namespace. 新增 context: ...... contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes - context: cluster: kubernetes namespace: test user: kubernetes-admin name: test - context: # 新增 context cluster: direct-cluster user: kubernetes-admin name: direct ... 参考:\nhttps://kubernetes.io/zh-cn/docs/concepts/configuration/organize-cluster-access-kubeconfig/ 什么是k8s上下文？kubeconfig配置文件讲解-CSDN博客 ","date":"2024-01-01T17:31:05+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/%E4%BD%BF%E7%94%A8kubeconfig%E7%AE%A1%E7%90%86%E9%9B%86%E7%BE%A4/cover1.webp","permalink":"/p/%E4%BD%BF%E7%94%A8-kubeconfig-%E7%AE%A1%E7%90%86%E9%9B%86%E7%BE%A4/","title":"使用 kubeconfig 管理集群"},{"content":"k8s 集群配置 IP Host 配置 11.0.1.150 master1 (keepalived+haproxy) 2C 4G 30G 11.0.1.148 master2 (keepalived+haproxy) 2C 4G 30G 11.0.1.149 node1 2C 4G 30G 理解 kubeconfig 本文是使用 kubectl 来管理 k8s 集群, 所以你需要知道 kubeconfig 的配置结构, 也可以直接 copy-paste\nkubectl 管理优势: 可通过切换 context 上下文, 更换操作集群 =\u0026gt; 可管理集群联邦\n推荐: 理解/使用 kubeconfig 管理集群 \u0026ndash; CSDN\n思路整理: 首先理清思路, 要使用 wsl 管理远程 k8s 集群\n确保 wsl 能够连接到 k8s 集群 要管理 k8s 集群, 就需要在 wsl 上安装 kubectl, 不再需要 kubeadm 与 kubelet 要想使用 kubectl 管理集群, 就需要集群的 kubeconfig 与 集群证书 在 wsl 上安装 kubectl # 安装依赖 apt install apt-transport-https ca-certificates -y apt install vim lsof net-tools zip unzip tree wget curl bash-completion pciutils gcc make lrzsz tcpdump bind9-utils -y # 编辑镜像源文件，文件末尾加入阿里云k8s镜像源配置 echo \u0026#39;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main\u0026#39; \u0026gt;\u0026gt; /etc/apt/sources.list #更新证书 curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add #更新源 apt update # 查看 kubeadm 版本 apt-cache madison kubeactl | grep 1.28 apt-get install -y kubectl=1.28.1-00 配置自动补全 apt install bash-completion -y cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ~/.profile alias k=\u0026#39;kubectl\u0026#39; source \u0026lt;(kubectl completion bash) complete -F __start_kubectl k EOF source ~/.profile 拷贝 kubeconfig 登到 k8s 集群的 master 节点, 把 kubeconfig 拷贝到 wsl ssh 免密登录\necho \u0026#34;172.28.18.117 wsl\u0026#34; \u0026gt;\u0026gt; /etc/hosts # 测试链接 ping wsl # 拷贝公钥 ssh-copy-id -i ~/.ssh/id_rsa.pub root@wsl 如果有安全需要的话, 就不要把建立 wsl 与 master 的免密登录\n拷贝 kubeconfig\nssh root@wsl mkdir /root/.kube scp /root/.kube/config root@wsl:/root/.kube/config 测试 kubectl root@MKT:~# k get po -A NAMESPACE NAME READY STATUS RESTARTS AGE calico-apiserver calico-apiserver-69fc754d76-9w7p6 1/1 Running 1 (17m ago) 46h calico-apiserver calico-apiserver-69fc754d76-kdxt4 1/1 Running 1 (21m ago) 46h calico-system calico-kube-controllers-b9dcc57c4-gzlhk 1/1 Running 1 (21m ago) 46h calico-system calico-node-5jckf 1/1 Running 1 (17m ago) 46h calico-system calico-node-gq882 1/1 Running 1 (21m ago) 46h calico-system calico-node-zksvz 1/1 Running 1 (21m ago) 45h calico-system calico-typha-5cdb9d5d59-tnjzh 1/1 Running 1 (17m ago) 46h calico-system calico-typha-5cdb9d5d59-z7tnk 1/1 Running 1 (21m ago) 45h calico-system csi-node-driver-j4648 2/2 Running 2 (21m ago) 46h calico-system csi-node-driver-jf548 2/2 Running 2 (21m ago) 45h calico-system csi-node-driver-p9b8p 2/2 Running 2 (17m ago) 46h kube-system coredns-66f779496c-n4lwr 1/1 Running 1 46h kube-system coredns-66f779496c-tmjx9 1/1 Running 1 (21m ago) 46h kube-system etcd-master1 1/1 Running 3 (21m ago) 46h kube-system etcd-master2 1/1 Running 1 (21m ago) 45h kube-system kube-apiserver-master1 1/1 Running 3 (21m ago) 46h kube-system kube-apiserver-master2 1/1 Running 1 (21m ago) 45h kube-system kube-controller-manager-master1 1/1 Running 5 (21m ago) 46h kube-system kube-controller-manager-master2 1/1 Running 2 (20m ago) 45h kube-system kube-proxy-9tjjg 1/1 Running 1 (17m ago) 46h kube-system kube-proxy-chcc5 1/1 Running 1 (21m ago) 45h kube-system kube-proxy-mrp2v 1/1 Running 1 (21m ago) 46h kube-system kube-scheduler-master1 1/1 Running 5 (21m ago) 46h kube-system kube-scheduler-master2 1/1 Running 1 (21m ago) 45h tigera-operator tigera-operator-55585899bf-tcdhv 1/1 Running 4 (17m ago) 46h 参考:\nhttps://kubernetes.io/zh-cn/docs/concepts/configuration/organize-cluster-access-kubeconfig/ ","date":"2024-01-01T16:10:25+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/%E4%BD%BF%E7%94%A8wsl%E7%AE%A1%E7%90%86k8s%E9%9B%86%E7%BE%A4/cover.jpg","permalink":"/p/%E4%BD%BF%E7%94%A8-wsl-%E7%AE%A1%E7%90%86-k8s-%E9%9B%86%E7%BE%A4/","title":"使用 wsl 管理 k8s 集群"},{"content":"写时复制 dockerfile 一条指令对应一层, 最多 127 层\n从 busybox 入手理解 docker 镜像层的写时复制\ndockerfile:\nFROM busybox RUN mkdir -p /tmp/foo # 新建一个 100M 的文件 RUN dd if=/dev/zero of=/tmp/foo/bar bs=1M count=100 # 删除文件 RUN rm /tmp/foo/bar 一条指令对应 1 层: 4 层\n$ docker build -t busybox:copy-on-write -f dockerfile1 . $ docker images | grep busybox busybox copy-on-write 1f30f7a048eb About a minute ago 106MB busybox latest beae173ccac6 2 years ago 1.24MB 看得出来就算镜像删除的文件, 下层创建的不会消失, 删除指令创建的层只会叠加上去\n使用 docker inspect 查看具体的层信息\nbusybox:latest\ndocker inspect busybox:latest ...... \u0026#34;RootFS\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;Layers\u0026#34;: [ \u0026#34;sha256:82ae998286b2bba64ce571578647adcabef93d53867748d6046cc844ff193a83\u0026#34; ] }, ...... busybox:copy-on-write\ndocker inspect busybox:copy-on-write ...... \u0026#34;RootFS\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;Layers\u0026#34;: [ \u0026#34;sha256:82ae998286b2bba64ce571578647adcabef93d53867748d6046cc844ff193a83\u0026#34;, \u0026#34;sha256:f70c1ce3ef10664a7aacf002afd20aa16f56bf98a83729b640c97363b10d329c\u0026#34;, \u0026#34;sha256:3c38ae07f686f954897d06c4c56e65e17d1be5843f2c02e8f2d6734b4978dd83\u0026#34;, \u0026#34;sha256:69d9b35636c7fa1f62c4bfe15f3c65f247553b122fe38af17dc34666f8cb2314\u0026#34; ] }, ...... 可以看到 busybox:copy-on-write 明显增加了层数\n制作精简镜像方法: 串联 Dockerfile 指令 选用更小的镜像 压缩镜像 案例: 二进制部署 redis\n多条命令构建 FROM ubuntu:trusty ENV VER 7.2.3 # 安装依赖 RUN apt update RUN apt install wget make gcc -y # 获取源码包 RUN wget https://download.redis.io/releases/redis-$VER.tar.gz RUN tar zxvf redis-$VER.tar.gz WORKDIR redis-$VER # 编译源码包 RUN make RUN mkdir /usr/local/redis RUN make install PREFIX=/usr/local/redis RUN apt-get clean \u0026amp;\u0026amp; apt-get remove -y wget gcc make # 启动 redis CMD [\u0026#34;/usr/local/redis/redis-server\u0026#34;, \u0026#34;/usr/local/redis/redis.conf\u0026#34;] 构造镜像 docker build -t redis:many-lines-ubuntu -f dockerfile2 . ROOTFS ...... \u0026#34;RootFS\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;Layers\u0026#34;: [ \u0026#34;sha256:f2fa9f4cf8fd0a521d40e34492b522cee3f35004047e617c75fadeb8bfd1e6b7\u0026#34;, \u0026#34;sha256:30d3c4334a2379748937816c01f5c972a8291a5ccc958d6b33d735457a16196e\u0026#34;, \u0026#34;sha256:83109fa660b2ed9307948505abd3c1f24c27c64009691067edb765bd3714b98d\u0026#34;, \u0026#34;sha256:87c240db20b96e01aa53111dbf1a71b54095e95f2bb32076a7aee2de148aaaf1\u0026#34;, \u0026#34;sha256:c45a2d4a77a5eb88bd2bfaaa691468eade520c8da9b3d39d30813e9882ff7b0b\u0026#34;, \u0026#34;sha256:65faa09184aa345eeace30548ad9a337e4582f9b23d8a0871ad8a5be08fe6c09\u0026#34;, \u0026#34;sha256:af54af5d0e940764f3cd2b78d5ab121b2dba5eb1f527e81f8c70c3deeddf15b9\u0026#34;, \u0026#34;sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\u0026#34;, \u0026#34;sha256:1819a309db341aed9bc39e040a3695151d2b33c9d1d267069d6f3918f3871a1d\u0026#34;, \u0026#34;sha256:e54f0072570654b50dd413197f5d921da283c90689d79336ed0c313112d3a591\u0026#34;, \u0026#34;sha256:b6499beec6f2099e86807b4fee915170820834dfafae3ca324309db26e5660fb\u0026#34;, \u0026#34;sha256:14a95f408690a544c45387595381902f921eb0356a53f8a73495198dc113b8a1\u0026#34;, \u0026#34;sha256:a545e5b1525c37272eb70af26d8baab872f592c5f71ba934353a7104f0680e21\u0026#34; ] }, ...... 由于我们写了写多条 RUN 指令, 所以镜像 ROOTFS 极速增长\n一条命令构建 ubuntu FROM ubuntu:trusty ENV VER 7.2.3 RUN apt update \u0026amp;\u0026amp; \\ apt install wget make gcc -y \u0026amp;\u0026amp; \\ wget https://download.redis.io/releases/redis-$VER.tar.gz \u0026amp;\u0026amp; \\ tar zxvf redis-$VER.tar.gz \u0026amp;\u0026amp; cd redis-$VER \u0026amp;\u0026amp; \\ make \u0026amp;\u0026amp; mkdir /usr/local/redis \u0026amp;\u0026amp; \\ make install PREFIX=/usr/local/redis \u0026amp;\u0026amp; \\ mkdir -p /usr/local/redis/{conf,bin} \u0026amp;\u0026amp; \\ cp redis.conf /usr/local/redis/conf \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; apt-get remove -y wget gcc make # 启动 redis CMD [\u0026#34;/usr/local/redis/bin/redis-server\u0026#34;, \u0026#34;/usr/local/redis/conf\u0026#34;] 构造镜像 docker build -t redis:one-line-ubuntu -f dockerfile2 . ROOTFS docker inspect redis:one-line-ubuntu ...... \u0026#34;RootFS\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;Layers\u0026#34;: [ \u0026#34;sha256:f2fa9f4cf8fd0a521d40e34492b522cee3f35004047e617c75fadeb8bfd1e6b7\u0026#34;, \u0026#34;sha256:30d3c4334a2379748937816c01f5c972a8291a5ccc958d6b33d735457a16196e\u0026#34;, \u0026#34;sha256:83109fa660b2ed9307948505abd3c1f24c27c64009691067edb765bd3714b98d\u0026#34;, \u0026#34;sha256:72caaf8c025e80d97674e4d68ac4628b8dabab70dc9b73ef817c51b694c45daa\u0026#34; ] }, ...... 可以看出指令数减少, ROOTFS 层数也明显减少\n更改基础镜像为 debian 巨坑: debian 与 alpine 的镜像在切换国内时问题非常多, 建议直接使用原始镜像的软件源(确保能够访问外网)\n问题概览:\ndebian 低版本切换阿里源时, 不支持 https: 需要将 http 软连接为 https 显示 gpg key 错误时: wget http://http.us.debian.org/debian/pool/main/d/debian-archive-keyring/debian-archive-keyring_2023.4_all.deb \u0026amp;\u0026amp; dpkg -i debian-archive-keyring_2023.4_all.deb alpline 与 debian 安装的 gcc 可能会出现版本问题, 需要你更换 gcc 版本 FROM debian ENV VER 7.2.3 RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install wget make gcc -y \u0026amp;\u0026amp; \\ wget https://download.redis.io/releases/redis-$VER.tar.gz \u0026amp;\u0026amp; \\ tar zxvf redis-$VER.tar.gz \u0026amp;\u0026amp; cd redis-$VER \u0026amp;\u0026amp; \\ make \u0026amp;\u0026amp; mkdir /usr/local/redis \u0026amp;\u0026amp; \\ make install PREFIX=/usr/local/redis \u0026amp;\u0026amp; \\ mkdir -p /usr/local/redis/{conf,bin} \u0026amp;\u0026amp; \\ cp redis.conf /usr/local/redis/conf \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; apt-get remove -y wget gcc make # 启动 redis CMD [\u0026#34;/usr/local/redis/bin/redis-server\u0026#34;, \u0026#34;/usr/local/redis/conf\u0026#34;] 构造镜像 docker build -t redis:one-line-debian -f dockerfile2 . ROOTFS docker inspect redis:one-line-debian ...... \u0026#34;RootFS\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;Layers\u0026#34;: [ \u0026#34;sha256:ae134c61b154341a1dd932bd88cb44e805837508284e5d60ead8e94519eb339f\u0026#34;, \u0026#34;sha256:98547bcc9e3b713c48a4f5ca069fd442eaa71bc61b68c9a3c00a4af0d73cbe2d\u0026#34; ] }, ...... 可以看出将多条指令合并书写能够有效减少 ROOTFS 层数\n使用分段构造 多段构造, 在到最后一段, 有小镜像承接, 前面段的内容不会保留下来\n推荐镜像:\n镜像 说明 scrach 空镜像, 不带任何调试工具 busybox 小镜像, 带有基础调试工具 因为我们需要解压 tar 包, scrath 没有解压工具, 我们使用 busybox 演示\n使用 busybox 作为基础镜像\nFROM ubuntu:trusty as builder ENV VER 7.2.3 RUN apt update \u0026amp;\u0026amp; \\ apt install wget make gcc -y \u0026amp;\u0026amp; \\ wget https://download.redis.io/releases/redis-$VER.tar.gz \u0026amp;\u0026amp; \\ tar zxvf redis-$VER.tar.gz \u0026amp;\u0026amp; cd redis-$VER \u0026amp;\u0026amp; \\ make \u0026amp;\u0026amp; mkdir /usr/local/redis \u0026amp;\u0026amp; \\ make install PREFIX=/usr/local/redis \u0026amp;\u0026amp; \\ cp redis.conf /usr/local/redis/conf \u0026amp;\u0026amp; \\ apt-get clean \u0026amp;\u0026amp; apt-get remove -y wget gcc make RUN tar cvf /123.tar /usr/local/redis/ FROM busybox COPY --from=builder /123.tar / RUN tar xvf /123.tar -C /usr/local/ # 启动 redis CMD [\u0026#34;/usr/local/redis/bin/redis-server\u0026#34;, \u0026#34;/usr/local/redis/conf\u0026#34;] 构造镜像 补充: 如果你遇到 Dockerfile 构造时的错误时, 可以加上 --progress=plain --no-cache查看详细构建日志\ndocker build -t redis:with-busybox -f dockerfile7 . --progress=plain ROOTFS docker inspect redis:with-busybox ...... \u0026#34;RootFS\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;Layers\u0026#34;: [ \u0026#34;sha256:82ae998286b2bba64ce571578647adcabef93d53867748d6046cc844ff193a83\u0026#34;, \u0026#34;sha256:335bc67496eb30fe68fe25e6c8f97cb6b74d9989a1c6b24012971097bf22a1a3\u0026#34;, \u0026#34;sha256:abf10e66d6bddcb7e00f41707d71515b1927b1984889aa13ca6b232f9fb78b27\u0026#34; ] }, ...... 镜像压缩 了解即可 docker squash已不在维护\n安装 wget https://github.com/jwilder/docker-squash/releases/download/v0.2.0/docker-squash-linux-amd64-v0.2.0.tar.gz tar xzvf docker-squash-linux-amd64-v0.2.0.tar.gz -C /usr/local/bin/ 使用 root@MKT:~/goproject/httppod/tmp# docker save redis:many-lines-ubuntu | docker-squash -verbose -t redis:squash | docker load Loading export from STDIN using /tmp/docker-squash272131964 for tempdir Loaded image w/ 13 layers - redis (1 tags) Extracting layers... - /tmp/docker-squash272131964/72c5745a74e809afa1c8bbb09164a25a99971cd6ebb60bf9a3c7c399932e2d92/layer.tar - /tmp/docker-squash272131964/e0d139e2e59aa8505f205730d7bd8924bf1289034504be42b618c284420cb9c2/layer.tar - /tmp/docker-squash272131964/10bc199db63a91cf1c3d75fddb1083740e09115320801e0e34f517f8160bd326/layer.tar - /tmp/docker-squash272131964/228aa3e189d788785b2d8c129b6bbf1553ad34e141608706f2d0459d5a1be035/layer.tar - /tmp/docker-squash272131964/42f6beebfd61e1ff415ab2237fdb49bd34dee9f319a1d49e2ca525dc18823e5d/layer.tar - /tmp/docker-squash272131964/45ce59804d1f796310cf5ba3e5878be2e63d644ba271c8d33a315230e737324f/layer.tar - /tmp/docker-squash272131964/462272b123320a876d404d41dac9f20ecf24140f22d979896cc2d0bfa487bae2/layer.tar - /tmp/docker-squash272131964/63b9671bfe8674a1e65dfbb88881ddb1c214eabf89f6ed98e3f0e1deaca9bd8f/layer.tar - /tmp/docker-squash272131964/075972677a1d485c0ba03f4d3b2fb6e3c39f273628f6f0d702149faa4e2831bd/layer.tar - /tmp/docker-squash272131964/88f0016636d34b6a24606c2db4d7ea26c0a02d830383e46aee831c2e1db897a9/layer.tar - /tmp/docker-squash272131964/a8626ef4bd7f2ec2fde454e5263f09eea4d369983ef770abb470f1688baa6681/layer.tar - /tmp/docker-squash272131964/a8ea63a42a3c2e60d8738e8bd81be0502d4bc6ecc827c8e472e171d4abf7e5cf/layer.tar - /tmp/docker-squash272131964/b795b8b69dcd58cbf17a234cba5e8d1de61da850032d781ae92b3ba9eb0fdb69/layer.tar Inserted new layer 2619184f193f after 228aa3e189d7 - 228aa3e189d7 -\u0026gt; 2619184f193f /bin/sh -c #(squash) from 228aa3e189d7 - 075972677a1d - 10bc199db63a - a8626ef4bd7f - 88f0016636d3 - e0d139e2e59a - 63b9671bfe86 - 45ce59804d1f - 462272b12332 - b795b8b69dcd - 42f6beebfd61 - a8ea63a42a3c - 72c5745a74e8 Squashing from 2619184f193f into 2619184f193f - Deleting whiteouts for layer 075972677a1d - Deleting whiteouts for layer 10bc199db63a Removing extracted layers Tagging 2619184f193f as redis:squash Tarring new image to STDOUT Done. New image created. - 228aa3e189d7 54.036378 years 206.1 MB - 2619184f193f 17 seconds /bin/sh -c #(squash) from 228aa3e189d7 370.1 MB Removing tempdir /tmp/docker-squash272131964 Loaded image: redis:many-lines-ubuntu 总结 镜像大小\nroot@MKT:~/goproject/httppod/tmp# docker images REPOSITORY TAG IMAGE ID CREATED SIZE redis with-busybox a26dc68d9da9 17 minutes ago 65.1MB busybox copy-on-write e71e45a955a2 26 minutes ago 109MB redis many-lines-ubuntu cf3ded3f5a5f 43 minutes ago 565MB redis one-line-debian ac157643d4a6 About an hour ago 669MB redis one-line-ubuntu 86be0695bcf2 2 hours ago 562MB 精简镜像方式:\n使用单条指令替换多条指令 使用更小的基础镜像 使用镜像压缩工具 参考:\n容器化 | 使用 Alpine 构建 Redis 镜像 - RadonDB - 博客园 (cnblogs.com) 基于alpine用dockerfile创建的nginx镜像 - 一本正经的搞事情 - 博客园 (cnblogs.com) redis二进制安装_二进制安装redis-CSDN博客 apt get - Debian 8 Jessie archive.debian.org GPG error KEYEXPIRED since 2022-11-19 (what now?) - Stack Overflow ","date":"2024-01-01T15:01:35+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/%E7%B2%BE%E7%AE%80%E9%95%9C%E5%83%8F/cover.jpg","permalink":"/p/dockerfile-%E5%88%B6%E4%BD%9C%E7%B2%BE%E7%AE%80%E9%95%9C%E5%83%8F/","title":"Dockerfile: 制作精简镜像"},{"content":"问题场景: k8s 1.28.1\n集群后期新增 vip\napiserver 证书不支持 vip\n第二个 master 节点想要加入集群, 但是在 etcd 健康检查时, 实现 vip 不在 apiserver 证书范围内\n[kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [check-etcd] Checking that the etcd cluster is healthy error execution phase check-etcd: could not retrieve the list of etcd endpoints: Get \u0026#34;https://11.0.1.100:16443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd%2Ctier%3Dcontrol-plane\u0026#34;: tls: failed to verify certificate: x509: certificate is valid for 10.96.0.1, 11.0.1.150, not 11.0.1.100 To see the stack trace of this error execute with --v=5 or higher 问题分析: 说明 api-server 的证书没有添加 11.0.1.100\n问题解决: 查看 apiserver 证书支持的 ip 或 host openssl x509 -noout -text -in /etc/kubernetes/pki/apiserver.crt 输出: X509v3 Subject Alternative Name: DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:master1, IP Address:10.96.0.1, IP Address:11.0.1.150 说明当前 apiserver 不支持 vip 11.0.1.100 的连接\n使用 openssl 生成证书: mkdir /tmp/bak cp /etc/kubernetes/pki/ /tmp/bak/ -r # 生成密钥对 cd /etc/kubernetes/pki/ openssl genrsa -out apiserver.key 2048\t# 新增 apiserver.ext文件，包含所有的地址列表，以及新增地址 subjectAltName = DNS:wudang,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP:10.96.0.1, IP:11.0.1.150, IP:11.0.1.100 # 生成 openssl req -new -key apiserver.key -subj \u0026#34;/CN=kube-apiserver,\u0026#34; -out apiserver.csr 再次查看 apiserver 证书支持的 ip 或 host openssl x509 -noout -text -in apiserver.crt 输出: X509v3 extensions: X509v3 Subject Alternative Name: DNS:wudang, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:11.0.1.150, IP Address:11.0.1.100 可以看到 11.0.1.100 已经成功加上去了\n再次尝试将 master 加点加入 root@ubuntu:/etc/kubernetes/pki# kubeadm join 11.0.1.150:6443 --token iwqftg.rs9wydqac98ecqbv --discovery-token-ca-cert-hash sha256:698fef4be22b563ce3ae350971e8ca1302488eda76148df5c210a03ce29c0b1a --control-plane --certificate-key c994991c3445a3dc03fbe4f0d8794e8e51946a2b44c920c9a74fa5941b03261d [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [preflight] Running pre-flight checks before initializing the new control plane instance [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; W1230 19:00:20.797222 23382 checks.go:835] detected that the sandbox image \u0026#34;registry.aliyuncs.com/google_containers/pause:3.8\u0026#34; of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; as the CRI sandbox image. [download-certs] Downloading the certificates in Secret \u0026#34;kubeadm-certs\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [download-certs] Saving the certificates to the folder: \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master2] and IPs [10.96.0.1 11.0.1.151 11.0.1.100] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [localhost master2] and IPs [11.0.1.151 127.0.0.1 ::1] [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [localhost master2] and IPs [11.0.1.151 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Valid certificates and keys now exist in \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Using the existing \u0026#34;sa\u0026#34; key [kubeconfig] Generating kubeconfig files [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; W1230 19:00:21.802963 23382 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file W1230 19:00:22.105107 23382 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file W1230 19:00:22.181303 23382 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [check-etcd] Checking that the etcd cluster is healthy [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... [etcd] Announced new etcd member joining to the existing etcd cluster [etcd] Creating static Pod manifest for \u0026#34;etcd\u0026#34; [etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s The \u0026#39;update-status\u0026#39; phase is deprecated and will be removed in a future release. Currently it performs no operation [mark-control-plane] Marking the node master2 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node master2 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] This node has joined the cluster and a new control plane instance was created: * Certificate signing request was sent to apiserver and approval was received. * The Kubelet was informed of the new secure connection details. * Control plane label and taint were applied to the new node. * The Kubernetes control plane instances scaled up. * A new etcd member was added to the local/stacked etcd cluster. To start administering your cluster from this node, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Run \u0026#39;kubectl get nodes\u0026#39; to see this node join the cluster. 新增的 master 节点成功加入集群\n参考 Kubernetes学习(解决x509 certificate is valid for xxx, not yyy) | Z.S.K.\u0026rsquo;s Records (izsk.me) 解决 Kubeadm 添加新 Master 节点到集群出现 ETCD 健康检查失败错误_error execution phase check-etcd: etcd cluster is -CSDN博客 https://cloud.tencent.com/developer/article/1692388 ","date":"2023-12-30T20:10:27+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/%E6%89%A9%E5%B1%95%20apiserver%20%E8%BF%9E%E6%8E%A5%E8%AE%A4%E8%AF%81%20ip/cover.png","permalink":"/p/%E6%89%A9%E5%B1%95-apiserver-%E8%BF%9E%E6%8E%A5%E8%AE%A4%E8%AF%81-ip-%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/","title":"扩展 apiserver 连接认证 ip, 证书更新"},{"content":"集群配置 配置清单 OS： ubuntu 20.04 kubernetes： 1.28.1 Container Runtime：Containerd 1.7.11 CRI: runc 1.10 CNI: cni-plugin 1.4 集群规划 IP Host 配置 11.0.1.147 master1 (keepalived+nginx) 2C 4G 30G 11.0.1.148 master2 (keepalived+nginx) 2C 4G 30G 11.0.1.149 node1 2C 4G 30G 11.0.1.150 node2 2C 4G 30G 11.0.1.151 node3 2C 4G 30G 集群网络规划 Pod 网络: 10.244.0.0/16 Service 网络: 10.96.0.0/12 Node 网络: 11.0.1.0/24 安装配置 keepalived # 在规划的 vip 节点 apt install keepalived -y # master1 cat \u0026gt; /etc/keepalived/keepalived.conf \u0026lt;\u0026lt; EOF ! Configuration File for keepalived global_defs { router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state MASTER interface ens33 virtual_router_id 50 priority 100 advert_int 1 authentication { auth_type PASS auth_pass root } virtual_ipaddress { 11.0.1.100 } } EOF # master2 cat \u0026gt; /etc/keepalived/keepalived.conf \u0026lt;\u0026lt; EOF ! Configuration File for keepalived global_defs { router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 50 nopreempt priority 70 advert_int 1 virtual_ipaddress { 11.0.1.100 } } EOF # 重启 keepalived systemctl restart keepalived.service \u0026amp;\u0026amp; systemctl enable keepalived.service systemctl status keepalived.service # 查看 vip ip a | grep 11.0.1.100 # 可停掉 master1 看 vip 是否会漂移到 master2 安装配置 nginx apt install nginx -y systemctl status nginx # 修改 nginx 配置文件 cat /etc/nginx/nginx.conf user user; worker_processes auto; pid /run/nginx.pid; include /etc/nginx/modules-enabled/*.conf; events { worker_connections 768; # multi_accept on; } #添加了stream 这一段，其他的保持默认即可 stream { log_format main \u0026#39;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent\u0026#39;; access_log /var/log/nginx/k8s-access.log main; upstream k8s-apiserver { server 11.0.1.147:6443; #master01的IP和6443端口 server 11.0.1.148:6443; #master02的IP和6443端口 } server { listen 16443; #监听的是16443端口，因为nginx和master复用机器，所以不能是6443端口 proxy_pass k8s-apiserver; #使用proxy_pass模块进行反向代理 } } http { ## # Basic Settings ## sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; # server_tokens off; # server_names_hash_bucket_size 64; # server_name_in_redirect off; include /etc/nginx/mime.types; default_type application/octet-stream; ## # SSL Settings ## ssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3; # Dropping SSLv3, ref: POODLE ssl_prefer_server_ciphers on; ## # Logging Settings ## access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; ## # Gzip Settings ## gzip on; # gzip_vary on; # gzip_proxied any; # gzip_comp_level 6; # gzip_buffers 16 8k; # gzip_http_version 1.1; # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript; ## # Virtual Host Configs ## include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; } # 重启 nginx 服务 systemctl restart nginx \u0026amp;\u0026amp; systemctl enable nginx \u0026amp;\u0026amp; systemctl status nginx # 端口检查 netstat -lntup| grep 16443 配置高可用 ApiServer 可以使用 kubeadm init 初始化集群时, 指定高可用地址 command line kubeadm init \\ --apiserver-advertise-address=11.0.1.147 \\ --apiserver-bind-port=6443 \\ --control-plane-endpoint=11.0.1.100:16443 \\ # 指定 vip + nginx 端口 --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.28.1 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 yaml 方式初始化 kubeadm 初始化集群文件 Kubernetes-cluster.yaml:\napiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: # 将此处IP地址替换为主节点IP ETCD容器会试图通过此地址绑定端口 如果主机不存在则会失败 advertiseAddress: 11.0.1.147 bindPort: 6443 nodeRegistration: criSocket: unix:///run/containerd/containerd.sock imagePullPolicy: IfNotPresent name: master1 # 节点 hostname taints: null --- # controlPlaneEndpoint 可配置高可用的 ApiServer apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 controlPlaneEndpoint: 11.0.1.100:6443 # 使用 keepalived + nginx 的高可用地址 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: # 可使用外接 etcd 集群 local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 国内源 kind: ClusterConfiguration kubernetesVersion: 1.28.1 networking: dnsDomain: cluster.local # 增加配置 指定pod网段 podSubnet: \u0026#34;10.244.0.0/16\u0026#34; serviceSubnet: 10.96.0.0/12 scheduler: {} --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs # kubeproxy 使用 ipvs --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd 在已有集群上添加高可用地址 kubectl -n kube-system edit cm kubeadm-config # 把 controlPlaneEndpoint 修改为高可用地址即可 单 master 转 vip 高可用的坑:\n如果遇到加入新节点时, 出现 tls 相关报错, 显示连接 ip 不支持, 则需要更新 apiserver 证书\n","date":"2023-12-24T20:48:26+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/keepalived%2Bnginx%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8apiserver/cover.png","permalink":"/p/keepalived-nginx%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8apiserver/","title":"Keepalived+nginx实现高可用apiserver"},{"content":"集群配置 配置清单 OS： ubuntu 20.04 kubernetes： 1.28.1 Container Runtime：Containerd 1.7.11 CRI: runc 1.10 CNI: cni-plugin 1.4 集群规划 IP Hostname 配置 11.0.1.147 master1 2C 4G 30G 11.0.1.148 master2 2C 4G 30G 11.0.1.149 node1 2C 4G 30G 11.0.1.150 node2 2C 4G 30G 11.0.1.151 node3 2C 4G 30G 集群网络规划 Pod 网络: 10.244.0.0/16 Service 网络: 10.96.0.0/12 Node 网络: 11.0.1.0/24 环境初始化 主机配置 # 修改主机名 hostnamectl set-hostname master1 hostnamectl set-hostname master2 hostnamectl set-hostname node1 hostnamectl set-hostname node2 hostnamectl set-hostname node3 # 将节点加入 hosts cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/hosts 11.0.1.147 master1 11.0.1.148 master2 11.0.1.149 node1 11.0.1.150 node2 11.0.1.151 node3 EOF # 时间同步 timedatectl set-timezone Asia/Shanghai #安装chrony，联网同步时间 apt install chrony -y \u0026amp;\u0026amp; systemctl enable --now chronyd # 配置 ssh 免密登录 ssh-copy-id -i /root/.ssh/id_rsa.pub root@11.0.1.148 ssh-copy-id -i /root/.ssh/id_rsa.pub root@11.0.1.149 ssh-copy-id -i /root/.ssh/id_rsa.pub root@11.0.1.150 ssh-copy-id -i /root/.ssh/id_rsa.pub root@11.0.1.151 禁用 swap sudo swapoff -a \u0026amp;\u0026amp; sed -i \u0026#39;/swap/s/^/#/\u0026#39; /etc/fstab 安装 ipvs apt install -y ipset ipvsadm 调整内核参数 # 配置需要的内核模块 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF # 启动模块 sudo modprobe overlay sudo modprobe br_netfilter cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF # 是 sysctl 参数生效 sudo sysctl --system # 检验是否配置成功 lsmod | grep br_netfilter lsmod | grep overlay sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward # 配置 ipvs 内核参数 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/ipvs.conf ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack EOF # 内核加载 ipvs sudo modprobe ip_vs sudo modprobe ip_vs_rr sudo modprobe ip_vs_wrr sudo modprobe ip_vs_sh sudo modprobe nf_conntrack # 确认ipvs模块加载 lsmod |grep -e ip_vs -e nf_conntrack 安装 Containerd 二进制安装 containerd wget -c https://github.com/containerd/containerd/releases/download/v1.7.11/containerd-1.7.11-linux-amd64.tar.gz tar -xzvf containerd-1.7.11-linux-amd64.tar.gz #解压出来一个bin目录,containerd可执行文件都在bin目录里面 mv bin/* /usr/local/bin/ rm -rf bin #使用systemcd来管理containerd wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service mv containerd.service /usr/lib/systemd/system/ systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now containerd systemctl status containerd 安装 OCI Interface runc #安装runc #runc是容器运行时，runc实现了容器的init，run，create，ps...我们在运行容器所需要的cmd： curl -LO https://github.com/opencontainers/runc/releases/download/v1.1.10/runc.amd64 \u0026amp;\u0026amp; \\ install -m 755 runc.amd64 /usr/local/sbin/runc 安装 CNI plugins wget -c https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz #根据官网的安装步骤来，创建一个目录用于存放cni插件 mkdir -p /opt/cni/bin tar -xzvf cni-plugins-linux-amd64-v1.4.0.tgz -C /opt/cni/bin/ 修改 Containd 配置 #修改containerd的配置，因为containerd默认从k8s官网拉取镜像 #创建一个目录用于存放containerd的配置文件 mkdir -p /etc/containerd #把containerd配置导出到文件 containerd config default | sudo tee /etc/containerd/config.toml # 修改沙箱镜像 sed -i \u0026#39;s#sandbox_image = \u0026#34;registry.k8s.io/pause:3.8\u0026#34;#sandbox_image = \u0026#34;registry.aliyuncs.com/google_containers/pause:3.8\u0026#34;#\u0026#39; /etc/containerd/config.toml # 修改 cgroup 为 systemd sed -i \u0026#39;s#SystemdCgroup = false#SystemdCgroup = true#\u0026#39; /etc/containerd/config.toml # 配置镜像加速 sed -i \u0026#39;s#config_path = \u0026#34;\u0026#34;#config_path = \u0026#34;/etc/containerd/certs.d\u0026#34;#\u0026#39; /etc/containerd/config.toml 配置 Containerd 镜像源 # docker hub镜像加速 mkdir -p /etc/containerd/certs.d/docker.io cat \u0026gt; /etc/containerd/certs.d/docker.io/hosts.toml \u0026lt;\u0026lt; EOF server = \u0026#34;https://docker.io\u0026#34; [host.\u0026#34;https://dockerproxy.com\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] [host.\u0026#34;https://docker.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] [host.\u0026#34;https://reg-mirror.qiniu.com\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] [host.\u0026#34;https://registry.docker-cn.com\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] [host.\u0026#34;http://hub-mirror.c.163.com\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;] EOF # registry.k8s.io镜像加速 mkdir -p /etc/containerd/certs.d/registry.k8s.io tee /etc/containerd/certs.d/registry.k8s.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.k8s.io\u0026#34; [host.\u0026#34;https://k8s.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF # docker.elastic.co镜像加速 mkdir -p /etc/containerd/certs.d/docker.elastic.co tee /etc/containerd/certs.d/docker.elastic.co/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://docker.elastic.co\u0026#34; [host.\u0026#34;https://elastic.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF # gcr.io镜像加速 mkdir -p /etc/containerd/certs.d/gcr.io tee /etc/containerd/certs.d/gcr.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://gcr.io\u0026#34; [host.\u0026#34;https://gcr.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF # ghcr.io镜像加速 mkdir -p /etc/containerd/certs.d/ghcr.io tee /etc/containerd/certs.d/ghcr.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://ghcr.io\u0026#34; [host.\u0026#34;https://ghcr.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF # k8s.gcr.io镜像加速 mkdir -p /etc/containerd/certs.d/k8s.gcr.io tee /etc/containerd/certs.d/k8s.gcr.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://k8s.gcr.io\u0026#34; [host.\u0026#34;https://k8s-gcr.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF # mcr.m.daocloud.io镜像加速 mkdir -p /etc/containerd/certs.d/mcr.microsoft.com tee /etc/containerd/certs.d/mcr.microsoft.com/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://mcr.microsoft.com\u0026#34; [host.\u0026#34;https://mcr.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF # nvcr.io镜像加速 mkdir -p /etc/containerd/certs.d/nvcr.io tee /etc/containerd/certs.d/nvcr.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://nvcr.io\u0026#34; [host.\u0026#34;https://nvcr.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF # quay.io镜像加速 mkdir -p /etc/containerd/certs.d/quay.io tee /etc/containerd/certs.d/quay.io/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://quay.io\u0026#34; [host.\u0026#34;https://quay.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF # registry.jujucharms.com镜像加速 mkdir -p /etc/containerd/certs.d/registry.jujucharms.com tee /etc/containerd/certs.d/registry.jujucharms.com/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://registry.jujucharms.com\u0026#34; [host.\u0026#34;https://jujucharms.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF # rocks.canonical.com镜像加速 mkdir -p /etc/containerd/certs.d/rocks.canonical.com tee /etc/containerd/certs.d/rocks.canonical.com/hosts.toml \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server = \u0026#34;https://rocks.canonical.com\u0026#34; [host.\u0026#34;https://rocks-canonical.m.daocloud.io\u0026#34;] capabilities = [\u0026#34;pull\u0026#34;, \u0026#34;resolve\u0026#34;, \u0026#34;push\u0026#34;] EOF #重启containerd systemctl restart containerd systemctl status containerd 创建容器确保 containerd 正确运行(可选) #拉取镜像，测试containerd是否能创建和启动成功 ctr i pull docker.io/library/nginx:alpine\t#能正常拉取镜像说明没啥问题 ctr images ls\t#查看镜像 ctr c create --net-host docker.io/library/nginx:alpine nginx #创建容器 ctr task start -d nginx\t#启动容器，正常说明containerd没啥问题 ctr containers ls #查看容器 ctr tasks kill -s SIGKILL nginx\t#终止容器 ctr containers rm nginx\t#删除容器 安装 kubeadm、kubelet、kubectl # 安装依赖 apt install apt-transport-https ca-certificates -y apt install vim lsof net-tools zip unzip tree wget curl bash-completion pciutils gcc make lrzsz tcpdump bind9-utils -y # 编辑镜像源文件，文件末尾加入阿里云k8s镜像源配置 echo \u0026#39;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main\u0026#39; \u0026gt;\u0026gt; /etc/apt/sources.list #更新证书 curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add #更新源 apt update # 查看 kubeadm 版本 apt-cache madison kubeadm | grep 1.28 apt-get install -y kubeadm=1.28.1-00 kubectl=1.28.1-00 kubelet=1.28.1-00 # kubelet 开机自启 systemctl enable kubelet 配置 crictl socket crictl config runtime-endpoint unix:///run/containerd.sock crictl config image-endpoint unix:///run/containerd/containerd.sock kubeadm init 一: 直接通过 kubeadm init 初始化集群 可提前拉取镜像 kubeadm config images list --kubernetes-version=v1.28.1 --image-repository=registry.aliyuncs.com/google_containers kubeadm config images pull --kubernetes-version=v1.28.1 --image-repository=registry.aliyuncs.com/google_containers 初始化集群 以下是安装是非高可用这并不影响高可用的配置, 我另一篇博客 keepalived+nginx实现高可用apiserver, 初始化集群与高可用的 apiserver 两步骤可以完全独立, 这两篇博客带你了解 kubeadm init 的多种方式\nkubeadm init \\ --apiserver-advertise-address=11.0.1.147 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.28.1 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 二: 通过加载配置文件初始化集群 获取集群初始化配置文件 kubeadm config print init-defaults \u0026gt;Kubernetes-cluster.yaml vim Kubernetes-cluster.yaml Kubernetes-cluster.yaml apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: # 将此处IP地址替换为主节点IP ETCD容器会试图通过此地址绑定端口 如果主机不存在则会失败 advertiseAddress: 11.0.1.147 bindPort: 6443 nodeRegistration: criSocket: unix:///run/containerd/containerd.sock imagePullPolicy: IfNotPresent name: master1 # 节点 hostname taints: null --- # controlPlaneEndpoint 可配置高可用的 ApiServer apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: # 可使用外接 etcd 集群 local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/google_containers # 国内源 kind: ClusterConfiguration kubernetesVersion: 1.28.1 networking: dnsDomain: cluster.local # 增加配置 指定pod网段 podSubnet: \u0026#34;10.244.0.0/16\u0026#34; serviceSubnet: 10.96.0.0/12 scheduler: {} --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs # kubeproxy 使用 ipvs --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd 获取或修改: kubectl -n kube-system edit cm kubeadm-config\n使用配置文件初始化集群 kubeadm init --config Kubernetes-cluster.yaml 复制 kubeconfig mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 加入节点 加入 工作节点 可直接复制 kubeadm init 后的 join 加入\nkubeadm join 11.0.1.147:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:7b465a19bae495131a16b51967a0c329bce9fe7d49136c641929eda69cfd6969 加入 master 如果 kubeadm init 后有加入 master 的命令直接复制就行\n如果没有就自己创建 control-plane 的 cert\n创建 cert-key $ kubeadm init phase upload-certs --upload-certs [upload-certs] Using certificate key: d38fbc73dc4c113409597a59d65ee66e4641ca220a463b1efeac9baa14f2924a 创建 token 也可以直接用 kubeadm init 产生的 token $ kubeadm token create --print-join-command kubeadm join 11.0.1.147:6443 --token m4l8th.81p28vmm5dh3nxl9 --discovery-token-ca-cert-hash sha256:7b465a19bae495131a16b51967a0c329bce9fe7d49136c641929eda69cfd6969 加入 master2 kubeadm join 11.0.1.147:6443 --token m4l8th.81p28vmm5dh3nxl9 --discovery-token-ca-cert-hash sha256:7b465a19bae495131a16b51967a0c329bce9fe7d49136c641929eda69cfd6969 --control-plane --certificate-key d38fbc73dc4c113409597a59d65ee66e4641ca220a463b1efeac9baa14f2924a 排错\n如何向Kubernetes集群中添加master节点（原集群只有一个master节点）\n[preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; error execution phase preflight: One or more conditions for hosting a new control plane instance is not satisfied. unable to add a new control plane instance to a cluster that doesn\u0026#39;t have a stable controlPlaneEndpoint address Please ensure that: * The cluster has a stable controlPlaneEndpoint address. * The certificates that must be shared among control plane instances are provided. To see the stack trace of this error execute with --v=5 or higher 说明 apiserver 没有绑定到固定 IP, 可以在 master1\nkubectl -n kube-system edit cm kubeadm-config # 修改 data 中 controlPlaneEndpoint 为一个静态 IP(要确保静态 IP 能够访问 apiserver 注意证书, 后期可通过 keepalived/nginx 配置 apiserver 的高可用, root@ubuntu:~# kubeadm join 11.0.1.147:6443 --token m4l8th.81p28vmm5dh3nxl9 --discovery-token-ca-cert-hash sha256:7b465a19bae495131a16b51967a0c329bce9fe7d49136c641929eda69cfd6969 --control-plane --certificate-key d38fbc73dc4c113409597a59d65ee66e4641ca220a463b1efeac9baa14f2924a [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [preflight] Running pre-flight checks before initializing the new control plane instance [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; W1223 14:04:19.065758 16517 checks.go:835] detected that the sandbox image \u0026#34;registry.aliyuncs.com/google_containers/pause:3.8\u0026#34; of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \u0026#34;registry.aliyuncs.com/google_containers/pause:3.9\u0026#34; as the CRI sandbox image. [download-certs] Downloading the certificates in Secret \u0026#34;kubeadm-certs\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [download-certs] Saving the certificates to the folder: \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [localhost master2] and IPs [11.0.1.148 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [localhost master2] and IPs [11.0.1.148 127.0.0.1 ::1] [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master2] and IPs [10.96.0.1 11.0.1.148 11.0.1.147] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Valid certificates and keys now exist in \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Using the existing \u0026#34;sa\u0026#34; key [kubeconfig] Generating kubeconfig files [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [check-etcd] Checking that the etcd cluster is healthy [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... [etcd] Announced new etcd member joining to the existing etcd cluster [etcd] Creating static Pod manifest for \u0026#34;etcd\u0026#34; [etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s The \u0026#39;update-status\u0026#39; phase is deprecated and will be removed in a future release. Currently it performs no operation [mark-control-plane] Marking the node master2 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node master2 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] This node has joined the cluster and a new control plane instance was created: * Certificate signing request was sent to apiserver and approval was received. * The Kubelet was informed of the new secure connection details. * Control plane label and taint were applied to the new node. * The Kubernetes control plane instances scaled up. * A new etcd member was added to the local/stacked etcd cluster. To start administering your cluster from this node, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Run \u0026#39;kubectl get nodes\u0026#39; to see this node join the cluster. 配置命令行自动补全(可选) apt install bash-completion -y cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ~/.profile alias k=\u0026#39;kubectl\u0026#39; source \u0026lt;(kubectl completion bash) complete -F __start_kubectl k EOF source ~/.profile 安装 calico kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml vi custom-resources.yaml # This section includes base Calico installation configuration. # For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.Installation apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: # Note: The ipPools section cannot be modified post-install. ipPools: - blockSize: 26 cidr: 10.244.0.0/16 # 与划分的 pod 网段一致 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all() --- # This section configures the Calico API server. # For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.APIServer apiVersion: operator.tigera.io/v1 kind: APIServer metadata: name: default spec: {} 验证集群 root@ubuntu:~# k get po -A NAMESPACE NAME READY STATUS RESTARTS AGE calico-apiserver calico-apiserver-66cb6b4b7f-8l67s 1/1 Running 0 57s calico-apiserver calico-apiserver-66cb6b4b7f-p8xs9 0/1 Running 0 57s calico-system calico-kube-controllers-86d48c97dc-vzzcd 1/1 Running 0 5m32s calico-system calico-node-29snn 1/1 Running 0 5m32s calico-system calico-node-cqrrf 1/1 Running 0 5m32s calico-system calico-node-gvpjn 1/1 Running 0 5m32s calico-system calico-node-wq4mh 1/1 Running 0 5m32s calico-system calico-node-xfvkw 1/1 Running 0 5m32s calico-system calico-typha-55fd77b9db-2x8sv 1/1 Running 0 5m24s calico-system calico-typha-55fd77b9db-4r98k 1/1 Running 0 5m33s calico-system calico-typha-55fd77b9db-qk7cm 1/1 Running 0 5m24s calico-system csi-node-driver-bhzpm 2/2 Running 0 5m32s calico-system csi-node-driver-bptcd 2/2 Running 0 5m32s calico-system csi-node-driver-g884s 2/2 Running 0 5m32s calico-system csi-node-driver-vm4p7 0/2 ContainerCreating 0 5m32s calico-system csi-node-driver-zgmds 2/2 Running 0 5m32s kube-system coredns-66f779496c-6fmh9 1/1 Running 0 17h kube-system coredns-66f779496c-p47zp 1/1 Running 0 17h kube-system etcd-master1 1/1 Running 2 17h kube-system etcd-master2 1/1 Running 0 16h kube-system kube-apiserver-master1 1/1 Running 2 17h kube-system kube-apiserver-master2 1/1 Running 0 16h kube-system kube-controller-manager-master1 1/1 Running 4 17h kube-system kube-controller-manager-master2 1/1 Running 1 (11h ago) 16h kube-system kube-proxy-bb2qd 1/1 Running 0 17h kube-system kube-proxy-c4zqw 1/1 Running 0 17h kube-system kube-proxy-cnwnl 1/1 Running 0 16h kube-system kube-proxy-mtgn6 1/1 Running 0 17h kube-system kube-proxy-tlgln 1/1 Running 0 17h kube-system kube-scheduler-master1 1/1 Running 4 17h kube-system kube-scheduler-master2 1/1 Running 1 (11h ago) 16h tigera-operator tigera-operator-55585899bf-mcs5f 1/1 Running 0 5m46s root@ubuntu:~# k get no NAME STATUS ROLES AGE VERSION master1 Ready control-plane 17h v1.28.1 master2 Ready control-plane 16h v1.28.1 node1 Ready \u0026lt;none\u0026gt; 17h v1.28.1 node2 Ready \u0026lt;none\u0026gt; 17h v1.28.1 node3 Ready \u0026lt;none\u0026gt; 17h v1.28.1 参考文章: 将 Docker Engine 节点从 dockershim 迁移到 cri-dockerd | Kubernetes 使用 kubeadm 引导集群 | Kubernetes ubuntu安装指定版本docker(包含官方/国内安装方法)_ubuntu 18.04 安装docker 20.10.7-CSDN博客 ubuntu 20.4安装k8s 1.24.0、1.28.0（使用containerd）_ubuntu containerd 安装-CSDN博客 kubeadm 部署k8s v1.28.3集群 - 小吉猫 - 博客园 (cnblogs.com) 如何向Kubernetes集群中添加master节点（原集群只有一个master节点） - 知乎 (zhihu.com) [containerd] 镜像加速_containerd 镜像加速-CSDN博客 ","date":"2023-12-24T15:08:15+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/基于 containerd 部署k8s 1.28.1 (Ubuntu 20.04)/cover.png","permalink":"/p/%E5%9F%BA%E4%BA%8E-containerd-%E9%83%A8%E7%BD%B2k8s-1.28.1-ubuntu-20.04/","title":"基于 Containerd 部署k8s 1.28.1 (Ubuntu 20.04)"},{"content":"主机规划 主机 IP node1 11.0.1.144 node2 11.0.1.145 node3 11.0.1.146 确保 Go 版本 1.15+\netcd 集群证书 安装 cfssl apt install golang-cfssl -y 生成默认证书 cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults crs \u0026gt; ca-crs.json 修改证书 root@ubuntu:~/tls# cat ca-config.json { \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;876000h\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;etcdha\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;876000h\u0026#34;, \u0026#34;usages\u0026#34;: [ \u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;client auth\u0026#34;, \u0026#34;server auth\u0026#34; ] } } } } root@ubuntu:~/tls# cat ca-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;etcd\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;xian\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;chengdu\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;system\u0026#34; } ] } 生成 CA 证书 cfssl gencert -initca ca-csr.json | cfssljson -bare ca 创建 etcd 证书签名请求（etcd-csr.json) root@ubuntu:~/tls# cat etcd-csr.json { \u0026#34;CN\u0026#34;: \u0026#34;etcd\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;127.0.0.1\u0026#34;, \u0026#34;11.0.1.144\u0026#34;, \u0026#34;11.0.1.145\u0026#34;, \u0026#34;11.0.1.146\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;etcdha\u0026#34; } ] } 使用 ca 证书签发 etcd 证书 cfssl gencert -ca ca.pem -ca-key ca-key.pem -config ca-config.json -profile etcdha etcd-csr.json | cfssljson -bare etcd root@ubuntu:~/tls# ls ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem etcd.csr etcd-csr.json etcd-key.pem etcd.pem 复制证书 mkdir /opt/etcd -p cp etcd*.pem ca*.pem /opt/etcd/ssl/ cd /opt/etcd/ssl # 确保事先有相应目录 scp *.pem root@11.0.1.145:/opt/etcd/ssl/ scp *.pem root@11.0.1.146:/opt/etcd/ssl/ etcd HA 部署 安装 etcd ETCD_VER=v3.5.11 # choose either URL GOOGLE_URL=https://storage.googleapis.com/etcd GITHUB_URL=https://github.com/etcd-io/etcd/releases/download DOWNLOAD_URL=${GOOGLE_URL} rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz rm -rf /tmp/etcd-download-test \u0026amp;\u0026amp; mkdir -p /tmp/etcd-download-test curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1 rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz /tmp/etcd-download-test/etcd --version /tmp/etcd-download-test/etcdctl version /tmp/etcd-download-test/etcdutl version mv /tmp/etcd-download-test/etcd /bin/ mv /tmp/etcd-download-test/etcdctl /bin/ mv /tmp/etcd-download-test/etcdutl /bin/ 分别在各个节点启动 etcd node1:\nnohup etcd --name infra0 \\ --data-dir=/tmp/etcd/infra0 \\ --listen-peer-urls https://11.0.1.144:2380 \\ --initial-advertise-peer-urls https://11.0.1.144:2380 \\ --listen-client-urls https://11.0.1.144:2379 \\ --advertise-client-urls https://11.0.1.144:2379 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-cluster infra0=https://11.0.1.144:2380,infra1=https://11.0.1.145:2380,infra2=https://11.0.1.146:2380 \\ --initial-cluster-state new \\ --client-cert-auth --trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --cert-file=/opt/etcd/ssl/etcd.pem \\ --key-file=/opt/etcd/ssl/etcd-key.pem \\ --peer-client-cert-auth --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --peer-cert-file=/opt/etcd/ssl/etcd.pem \\ --peer-key-file=/opt/etcd/ssl/etcd-key.pem 2\u0026gt;\u0026amp;1 \u0026gt; /var/log/infra0.log \u0026amp; node2:\nnohup etcd --name infra1 \\ --data-dir=/tmp/etcd/infra1 \\ --listen-peer-urls https://11.0.1.145:2380 \\ --initial-advertise-peer-urls https://11.0.1.145:2380 \\ --listen-client-urls https://11.0.1.145:2379 \\ --advertise-client-urls https://11.0.1.145:2379 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-cluster infra0=https://11.0.1.144:2380,infra1=https://11.0.1.145:2380,infra2=https://11.0.1.146:2380 \\ --initial-cluster-state new \\ --client-cert-auth --trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --cert-file=/opt/etcd/ssl/etcd.pem \\ --key-file=/opt/etcd/ssl/etcd-key.pem \\ --peer-client-cert-auth --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --peer-cert-file=/opt/etcd/ssl/etcd.pem \\ --peer-key-file=/opt/etcd/ssl/etcd-key.pem 2\u0026gt;\u0026amp;1 \u0026gt; /var/log/infra1.log \u0026amp; node3:\nnohup etcd --name infra2 \\ --data-dir=/tmp/etcd/infra2 \\ --listen-peer-urls https://11.0.1.146:2380 \\ --initial-advertise-peer-urls https://11.0.1.146:2380 \\ --listen-client-urls https://11.0.1.146:2379 \\ --advertise-client-urls https://11.0.1.146:2379 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-cluster infra0=https://11.0.1.144:2380,infra1=https://11.0.1.145:2380,infra2=https://11.0.1.146:2380 \\ --initial-cluster-state new \\ --client-cert-auth --trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --cert-file=/opt/etcd/ssl/etcd.pem \\ --key-file=/opt/etcd/ssl/etcd-key.pem \\ --peer-client-cert-auth --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --peer-cert-file=/opt/etcd/ssl/etcd.pem \\ --peer-key-file=/opt/etcd/ssl/etcd-key.pem 2\u0026gt;\u0026amp;1 \u0026gt; /var/log/infra2.log \u0026amp; 查看集群状态 root@ubuntu:/opt/etcd/ssl# etcdctl --endpoints=https://11.0.1.144:2379 --cacert /opt/etcd/ssl/ca.pem --cert /opt/etcd/ssl/etcd.pem --key /opt/etcd/ssl/etcd-key.pem member list -wtable +------------------+---------+--------+-------------------------+-------------------------+------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER | +------------------+---------+--------+-------------------------+-------------------------+------------+ | 68e936a39e332ee | started | infra1 | https://11.0.1.145:2380 | https://11.0.1.145:2379 | false | | 65d6166ada62ab60 | started | infra0 | https://11.0.1.144:2380 | https://11.0.1.144:2379 | false | | 8a036567f69e370a | started | infra2 | https://11.0.1.146:2380 | https://11.0.1.146:2379 | false | +------------------+---------+--------+-------------------------+-------------------------+------------+ etcd 集群备份与恢复 插入数据 root@ubuntu:~# etcdctl --endpoints=https://11.0.1.144:2379 --cacert /opt/etcd/ssl/ca.pem --cert /opt/etcd/ssl/etcd.pem --key /opt/etcd/ssl/etcd-key.pem put /test1 val1 OK root@ubuntu:~# etcdctl --endpoints=https://11.0.1.144:2379 --cacert /opt/etcd/ssl/ca.pem --cert /opt/etcd/ssl/etcd.pem --key /opt/etcd/ssl/etcd-key.pem put /test2 val2 OK root@ubuntu:~# etcdctl --endpoints=https://11.0.1.144:2379 --cacert /opt/etcd/ssl/ca.pem --cert /opt/etcd/ssl/etcd.pem --key /opt/etcd/ssl/etcd-key.pem put /test1/t1 val1/v1 OK root@ubuntu:~# etcdctl --endpoints=https://11.0.1.144:2379 --cacert /opt/etcd/ssl/ca.pem --cert /opt/etcd/ssl/etcd.pem --key /opt/etcd/ssl/etcd-key.pem get --prefix /test /test1 val1 /test1/t1 val1/v1 /test2 val2 备份 etcd 数据 etcdctl --endpoints=https://11.0.1.144:2379 --cacert /opt/etcd/ssl/ca.pem --cert /opt/etcd/ssl/etcd.pem --key /opt/etcd/ssl/etcd-key.pem snapshot save /tmp/snapshot.db scp /tmp/snapshot.db root@11.0.1.145:/tmp/ scp /tmp/snapshot.db root@11.0.1.146:/tmp/ 杀掉个节点 etcd 进程 ps -ef | grep infra | grep -v grep | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9 ps -ef | grep etcd 删除 etcd 数据 rm -rf /tmp/etcd 恢复 etcd 数据 node1:\nexport ETCDCTL_API=3 etcdctl snapshot restore /tmp/snapshot.db \\ --name infra0 \\ --data-dir=/tmp/etcd/infra0 \\ --initial-cluster infra0=https://11.0.1.144:2380,infra1=https://11.0.1.145:2379,infra2=https://11.0.1.146:2379 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls https://11.0.1.144:2380 node2:\nexport ETCDCTL_API=3 etcdctl snapshot restore /tmp/snapshot.db \\ --name infra1 \\ --data-dir=/tmp/etcd/infra1 \\ --initial-cluster infra0=https://11.0.1.144:2380,infra1=https://11.0.1.145:2380,infra2=https://11.0.1.146:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls https://11.0.1.145:2380 node3:\nexport ETCDCTL_API=3 etcdctl snapshot restore /tmp/snapshot.db \\ --name infra2 \\ --data-dir=/tmp/etcd/infra2 \\ --initial-cluster infra0=https://11.0.1.144:2380,infra1=https://11.0.1.145:2380,infra2=https://11.0.1.146:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls https://11.0.1.146:2380 重启 etcd node1:\nnohup etcd --name infra0 \\ --data-dir=/tmp/etcd/infra0 \\ --listen-peer-urls https://11.0.1.144:2380 \\ --listen-client-urls https://11.0.1.144:2379 \\ --advertise-client-urls https://11.0.1.144:2379 \\ --client-cert-auth --trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --cert-file=/opt/etcd/ssl/etcd.pem \\ --key-file=/opt/etcd/ssl/etcd-key.pem \\ --peer-client-cert-auth --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --peer-cert-file=/opt/etcd/ssl/etcd.pem \\ --peer-key-file=/opt/etcd/ssl/etcd-key.pem 2\u0026gt;\u0026amp;1 \u0026gt; /var/log/infra0.log \u0026amp; node2:\nnohup etcd --name infra1 \\ --data-dir=/tmp/etcd/infra1 \\ --listen-peer-urls https://11.0.1.145:2380 \\ --listen-client-urls https://11.0.1.145:2379 \\ --advertise-client-urls https://11.0.1.145:2379 \\ --client-cert-auth --trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --cert-file=/opt/etcd/ssl/etcd.pem \\ --key-file=/opt/etcd/ssl/etcd-key.pem \\ --peer-client-cert-auth --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --peer-cert-file=/opt/etcd/ssl/etcd.pem \\ --peer-key-file=/opt/etcd/ssl/etcd-key.pem 2\u0026gt;\u0026amp;1 \u0026gt; /var/log/infra1.log \u0026amp; node3:\nnohup etcd --name infra2 \\ --data-dir=/tmp/etcd/infra2 \\ --listen-peer-urls https://11.0.1.146:2380 \\ --listen-client-urls https://11.0.1.146:2379 \\ --advertise-client-urls https://11.0.1.146:2379 \\ --client-cert-auth --trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --cert-file=/opt/etcd/ssl/etcd.pem \\ --key-file=/opt/etcd/ssl/etcd-key.pem \\ --peer-client-cert-auth --peer-trusted-ca-file=/opt/etcd/ssl/ca.pem \\ --peer-cert-file=/opt/etcd/ssl/etcd.pem \\ --peer-key-file=/opt/etcd/ssl/etcd-key.pem 2\u0026gt;\u0026amp;1 \u0026gt; /var/log/infra2.log \u0026amp; 查看集群状态 root@ubuntu:/tmp# etcdctl --endpoints=https://11.0.1.146:2379 --cacert /opt/etcd/ssl/ca.pem --cert /opt/etcd/ssl/etcd.pem --key /opt/etcd/ssl/etcd-key.pem member list -wtable +------------------+-----------+--------+-------------------------+-------------------------+------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER | +------------------+-----------+--------+-------------------------+-------------------------+------------+ | 68e936a39e332ee | started | infra1 | https://11.0.1.145:2380 | https://11.0.1.145:2379 | false | | 65d6166ada62ab60 | unstarted | | https://11.0.1.144:2380 | | false | | 8a036567f69e370a | started | infra2 | https://11.0.1.146:2380 | https://11.0.1.146:2379 | false | +------------------+-----------+--------+-------------------------+-------------------------+------------+ root@ubuntu:/tmp# etcdctl --endpoints=https://11.0.1.146:2379 --cacert /opt/etcd/ssl/ca.pem --cert /opt/etcd/ssl/etcd.pem --key /opt/etcd/ssl/etcd-key.pem get --prefix / /test1 val1 /test1/t1 val1/v1 /test2 val2 infra0未启动成功, 其余两个节点正常, 原因未知, 待大佬指正\n参考:\netcd使用Cfssl生成自签证书(pem) - 西瓜君~ - 博客园 (cnblogs.com) [svc]证书各个字段的含义 - _毛台 - 博客园 (cnblogs.com) ","date":"2023-12-20T18:25:32+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/etcd高可用集群部署/cover.png","permalink":"/p/etcd%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","title":"Etcd高可用集群部署"},{"content":"Dockerfile 目标：易管理、少漏洞、镜像小、层级少、利用缓存。 Dockerfile 最佳实践 不要安装无效软件包。\n应简化镜像中同时运行的进程数，理想状况下，每个镜像应该只有一个进程。\n当无法避免同一镜像运行多进程时，应选择合理的初始化进程（initprocess）。\n最小化层级数\n最新的docker只有RUN，COPY，ADD创建新层，其他指令创建临时层，不会增加镜像大小。 比如EXPOSE指令就不会生成新层。 多条RUN命令可通过连接符连接成一条指令集以减少层数。 通过多段构建减少镜像层数。 把多行参数按字母排序，可以减少可能出现的重复参数，并且提高可读性。\n编写dockerfile的时候，应该把变更频率低的编译指令优先构建以便放在镜像底层以有效利用buildcache。\n复制文件时，每个文件应独立复制，这确保某个文件变更时，只影响改文件对应的缓存。\n理解 Dockerfile Dockerfile 可以看成构建镜像的一条条指令, 结合 docker 镜像打包理解, 一条指令大多对应一层, 追求共用层\n层的概念:\ndocker build 构建镜像 $ docker build [选项] \u0026lt;上下文路径/URL/-\u0026gt; 通过 docker build 构建镜像会自动扫描 Dockerfile 及其目录下的子目录, 通常将 Dockerfile 置于项目根目录, 如果文件文件过多, 镜像过程也会变慢\nDockerfile 常用指令 From 指定镜像, 默认从 docker hub 上拉去\nLABEL 用于为镜像添加元数据\n格式: LABEL \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; ... 配合 label filter 进行过滤\n$ docker images -f label=multi.label1=\u0026#34;value1\u0026#34; 注：\n使用LABEL指定元数据时，一条LABEL指定可以指定一或多条元数据，指定多条元数据时不同元数据\n之间通过空格分隔。推荐将所有的元数据通过一条LABEL指令指定，以免生成过多的中间镜像。\nENV 设置环境变量\nRUN 构建镜像时执行的命令\n格式： RUN [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;] 示例： RUN [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;] RUN apk update RUN [\u0026#34;/etc/execfile\u0026#34;, \u0026#34;arg1\u0026#34;, \u0026#34;arg1\u0026#34;] 注：RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，\n可以在构建时指定\u0026ndash;no-cache参数，如：docker build \u0026ndash;no-cache\nCMD 构建镜像后调用，也就是在容器启动时才进行调用。\n格式： CMD [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (执行可执行文件，优先) CMD [\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (设置了ENTRYPOINT，则直接调用ENTRYPOINT添加参数) CMD command param1 param2 (执行shell内部命令) 示例： CMD echo \u0026#34;This is a test.\u0026#34; | wc -l CMD [\u0026#34;/usr/bin/wc\u0026#34;,\u0026#34;--help\u0026#34;] 注：CMD不同于RUN，CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令。\nENTRYPOINT 镜像的第一个进程\n格式： ENTRYPOINT [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;] (可执行文件, 优先) ENTRYPOINT command param1 param2 (shell内部命令) 示例： FROM ubuntu ENTRYPOINT [\u0026#34;ls\u0026#34;, \u0026#34;/usr/local\u0026#34;] CMD [\u0026#34;/usr/local/tomcat\u0026#34;] 之后，docker run 传递的参数，都会先覆盖cmd,然后由cmd 传递给entrypoint ,做到灵活应用 注：ENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT，\n而docker run命令中指定的任何参数，都会被当做参数再次传递给CMD。\nDockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，\n而只执行最后的ENTRYPOINT指令。\n通常情况下，\tENTRYPOINT 与CMD一起使用，ENTRYPOINT 写默认命令，当需要参数时候 使用CMD传参\nDockerfile: ENTRYPOINT和CMD的区别 - 知乎 (zhihu.com)\nWORKDIR 工作目录，类似于cd命令\nADD 将本地文件添加到容器中，tar类型文件会自动解压(网络压缩资源不会被解压)，可以访问网络资源，类似wget\n格式： ADD \u0026lt;src\u0026gt;... \u0026lt;dest\u0026gt; ADD [\u0026#34;\u0026lt;src\u0026gt;\u0026#34;,... \u0026#34;\u0026lt;dest\u0026gt;\u0026#34;] 用于支持包含空格的路径 示例： ADD hom* /mydir/ # 添加所有以\u0026#34;hom\u0026#34;开头的文件 ADD hom?.txt /mydir/ # ? 替代一个单字符,例如：\u0026#34;home.txt\u0026#34; ADD test relativeDir/ # 添加 \u0026#34;test\u0026#34; 到 `WORKDIR`/relativeDir/ ADD test /absoluteDir/ # 添加 \u0026#34;test\u0026#34; 到 /absoluteDir/ COPY 与 ADD 相似, 但不能访问网络资源, 也不能解压, 以 ADD 相比, 行为简单, 因此可控, 推荐使用\nVOLUME 挂载\nVOLUME [\u0026#34;/path/to/dir\u0026#34;] 示例: VOLUME [\u0026#34;/data\u0026#34;] VOLUME [\u0026#34;/var/www\u0026#34;, \u0026#34;/var/log/apache2\u0026#34;, \u0026#34;/etc/apache2\u0026#34; 注：一个卷可以存在于一个或多个容器的指定目录，该目录可以绕过联合文件系统，并具有以下功能：\n卷可以容器间共享和重用 容器并不一定要和其它容器共享卷 修改卷后会立即生效 对卷的修改不会对镜像产生影响 卷会一直存在，直到没有任何容器在使用它 EXPOSE 暴露端口\nEXPOSE \u0026lt;port\u0026gt; [\u0026lt;port\u0026gt;...] 注：EXPOSE并不会让容器的端口访问到主机。要使其可访问，需要在docker run运行容器时通过-p来发布这些端口，或通过-P参数来发布EXPOSE导出的所有端口\n如果没有暴露端口，后期也可以通过-p 8080:80方式映射端口，但是不能通过-P形式映射\nUSER 指定运行容器的用户/组, 不指定默认 root, 指定的 Dockerfile 中其后的命令RUN、CMD、ENTRYPOINT都将使用该用户, 增强容器的安全性考虑, 做到 POLP(最小特权原则)。\n注： 镜像构建完成后，通过docker run运行容器时，可以通过-u参数来覆盖所指定的用户。\nARG 用于指定传递给构建运行时的变量(给dockerfile传参)，相当于构建镜像时可以在外部为里面传参\nFrom centos:7 ARG parameter VOLUME /usr/share/nginx RUN yum -y install $parameter EXPOSE 80 443 CMD nginx -g \u0026#34;daemon off;\u0026#34; # 可以这如下这样灵活传参 docker build --build-arg=parameter=net-tools -t nginx:01 . Dockerfile 模版 二进制构建 nginx # Base images 基础镜像 FROM centos #MAINTAINER 维护者信息 MAINTAINER Ai-feier #ENV 设置环境变量 ENV PATH /usr/local/nginx/sbin:$PATH #ADD 文件放在当前目录下，拷过去会自动解压 ADD nginx-1.8.0.tar.gz /usr/local/ ADD epel-release-latest-7.noarch.rpm /usr/local/ #RUN 执行以下命令 RUN rpm -ivh /usr/local/epel-release-latest-7.noarch.rpm RUN yum install -y wget lftp gcc gcc-c++ make openssl-devel pcre-devel pcre \u0026amp;\u0026amp; yum clean all RUN useradd -s /sbin/nologin -M www #WORKDIR 相当于cd WORKDIR /usr/local/nginx-1.8.0 RUN ./configure --prefix=/usr/local/nginx --user=www --group=www --with-http_ssl_module --with-pcre \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install RUN echo \u0026#34;daemon off;\u0026#34; \u0026gt;\u0026gt; /etc/nginx.conf #EXPOSE 映射端口 EXPOSE 80 #CMD 运行以下命令 CMD [\u0026#34;nginx\u0026#34;] apt 构建 nginx # Base images 基础镜像 FROM ubuntu RUN apt-get update \u0026amp;\u0026amp; apt-get install nginx -y \u0026amp;\u0026amp; apt-get clean CMD [\u0026#34;nginx\u0026#34;] 构建springboot应用 FROM openjdk:8-jre # jar包基于jdk ,war包基于tomcat WORKDIR /app ADD demo-0.0.1-SNAPSHOT.jar app.jar # 将上下文中 jar包复制到 /app目录下，并且重命名为app.jar EXPOSE 8081 # 暴露端口 ENTRYPOINT[ \u0026#34;java\u0026#34; , \u0026#34;-jar\u0026#34; ] # 启动应用固定命令 CMD [\u0026#34;app.jar\u0026#34;] # 动态传递jar包名 # CMD [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;java -jar app.jar\u0026#34;] 在 k8s 优雅终止时, 会发送 terminal 信号, 给应用进行优雅终止, 但是, /bin/sh 会无视这个信号, 最后直接被 kill 掉\nDockerfile 多段构造 目的: 将项目编译, 依赖等, 放到 Dockerfile 的早期构建, 最后留下一个干净的镜像, 有效减少镜像层级\n示例:\nFROM golang:1.16-alpine AS build RUN apkadd --no-cache git RUN go get github.com/golang/dep/cmd/dep COPY Gopkg.lock Gopkg.toml /go/src/project/ WORKDIR /go/src/project/ RUN dep ensure -vendor-only COPY . /go/src/project/ RUN go build -o /bin/project（只有这个二进制文件是产线需要的，其他都是waste） FROM scratch # 直接把镜像编译的包拷贝下来 COPY --from=build /bin/project /bin/project ENTRYPOINT [\u0026#34;/bin/project\u0026#34;] CMD [\u0026#34;--help\u0026#34;] 深入 [持续更新\u0026hellip;\u0026hellip;] Dockerfile: 制作精简镜像 \u0026ndash; CSDN 练习 [持续更新\u0026hellip;\u0026hellip;] Dockerfile: 制作精简镜像 \u0026ndash; CSDN ","date":"2023-12-18T02:12:07+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/Dockerfile/cover.png","permalink":"/p/dockerfile/","title":"Dockerfile"},{"content":"docker 网络模式 模式 说明 Null(\u0026ndash;net=None) 把容器放入独立网络空间, 但不做网络配置 Host 复用主机网络 Container 重用其他容器网络 Bridge docker 容器创建的默认模式, 容器通过 veth pair 连接到 docker0 网桥 veth pair 可跨 net namespace\nBridge docker 创建容器的默认模式\n创建 veth pair 将 veth pair 的一端连接到 docker0 veth pair 另一端设置为容器的 eth0 为容器明空间的 eth0 分配 ip 主机的 Iptables 规则: PREROUTING-A DOCKER ! -idocker0 -p tcp-m tcp\u0026ndash;dport 2333 -j DNAT \u0026ndash;to destination 172.17.0.2:22 解释: 如果你的协议是 tcp, 目标端口是 2333, 就转到 容器 172.17.0.2 的 22 端口\n为 Null 模式重新配置网络 Create network ns mkdir -p /var/run/netns find -L /var/run/netns -type l -delete Start nginx docker with non network mode docker run --network=none -d nginx Check corresponding pid docker ps|grep nginx docker inspect \u0026lt;containerid\u0026gt;|grep -i pid \u0026#34;Pid\u0026#34;: 876884, \u0026#34;PidMode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;PidsLimit\u0026#34;: null, Check network config for the container nsenter -t 558754 -n ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever Link network namespace export pid=558754 ln -s /proc/$pid/ns/net /var/run/netns/$pid ip netns list Check docker bridge on the host brctl show ip a 4: docker0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:35:40:d3:8b brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:35ff:fe40:d38b/64 scope link valid_lft forever preferred_lft forever Create veth pair ip link add A type veth peer name B Config A brctl addif docker0 A ip link set A up Config B SETIP=172.17.0.10 SETMASK=16 GATEWAY=172.17.0.1 ip link set B netns $pid ip netns exec $pid ip link set dev B name eth0 ip netns exec $pid ip link set eth0 up ip netns exec $pid ip addr add $SETIP/$SETMASK dev eth0 ip netns exec $pid ip route add default via $GATEWAY Check connectivity curl 172.17.0.10 容器的多主机间通信 Underlay 为容器的预留一端 IP, 主机就清楚其 IP 的路由, 则能解决主机间容器的互通\nOverlay 隧道模式, 为数据包多封装一层, 在对端主机解包\n","date":"2023-12-18T02:11:39+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/docker网络/cover.png","permalink":"/p/docker%E7%BD%91%E7%BB%9C/","title":"Docker网络"},{"content":"grep 语法格式:\ngrep [option] [pattern] [file1, file2] command | grep [option] [pattern] grep 参数:\n参数 含义 -v 不显示匹配信息 -i 忽略大小写 -n 显示行号 -r 递归搜索 -E 支持扩展正则表达式 (egrep) -F 不按正则表达式匹配 以下为不常用参数 -c 只显示匹配行总数 -w 匹配整词 -x 匹配整行 -l 只显示文件名, 不显示内容 -s 不显示错误信息 $ grep \u0026#39;py.*\u0026#39; file # grep 支持正则表达式 $ grep -E \u0026#34;python|PYTHON\u0026#34; file # 支持扩展正则表达式 $ grep -F \u0026#34;py.*\u0026#34; file # 会按原始语义搜索 egrep egrep 与 grep -E 等价\n$ grep -E \u0026#39;python|PYTHON\u0026#39; file $ egrep \u0026#39;python|PYTHON\u0026#39; file ","date":"2023-12-17T15:17:00+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/文本处理三剑客之grep/cover.png","permalink":"/p/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bgrep/","title":"文本处理三剑客之grep"},{"content":"sed 工作模式 sed 是流编辑器, 对标准输出文件逐行处理\n匹配 + 执行 : 默认会打印原始信息, 还会打印执行后的结果\n语法格式:\nstdout | sed [option] \u0026ldquo;pattern command\u0026rdquo; sed [optoin] \u0026ldquo;pattern command\u0026rdquo; pattern 是可选的\nsed 参数 选项 含义 -n 静默模式 -e 进行多条匹配规则 -f 使用文件, 文件为匹配规则 -r 支持扩展正则表达式 -i 直接修改文件内容 $ sed -n \u0026#39;/python/p\u0026#39; set.txt # p 为打印操作 $ sed -n -e \u0026#39;/python/p\u0026#39; -e \u0026#39;/PYTHON/p\u0026#39; sed.txt # 执行多条 # print.sed: /python/p $ sed -n -f print.sed sed.txt $ sed -n -r \u0026#39;/python|PYTHON/p\u0026#39; sed.txt # g: 全部修改; p: 打印到终端; -i 直接修改到文件 $ sed -i \u0026#39;s/love/like/g;p\u0026#39; sed.txt pattern 用法 匹配模式\n匹配模式 含义 10command 匹配第 10 行 10,20command 从第 10 行开始, 到第 20 行结束 10,+5command 从第 10 行开始, 到第 16 行结束 /pattern1/command (//d) 匹配 pattern1 的行 /pattern1/,/pattern2/command (//,//p) 匹配 pattern1 的行开始, 到 pattern2 的行结束 10,/pattern1/command 从第 10 行开始, 到匹配 pattern1 的行结束 /pattern1/,10command 从匹配 pattern1 的行开始, 到第 10 行结束 $ sed -n \u0026#39;10p\u0026#39; /etc/passwd $ sed -n \u0026#39;10,20p\u0026#39; /etc/passwd $ sed -n \u0026#39;10,+5p\u0026#39; /etc/passwd $ sed -n \u0026#39;/^root/p\u0026#39; /etc/passwd $ sed -n \u0026#39;/^root/,/^nginx/p\u0026#39; /etc/passwd $ sed -n \u0026#39;/4,/^nginx/p\u0026#39; /etc/passwd $ sed -n \u0026#39;/^root/,10p\u0026#39; /etc/passwd sed 编辑命令 打印 命令 含义 p 打印 $ sed \u0026#39;/^root/p\u0026#39; /etc/passwd 删除 命令 含义 d 删除行 $ sed -i \u0026#39;/\\/sbin\\/nologin/d\u0026#39; passwd $ sed -i \u0026#39;/^mail/,/^ftp/d\u0026#39; passwd 增加 命令 含义 a 行后追加 i 行前追加 r 从外部读入文件, 追加到行后 w 把结果写到对应文件 $ sed -i \u0026#39;/^root/,/^nginx/a AAAAAA\u0026#39; passwd # 匹配到的每一行后追加 $ sed -i \u0026#39;/^root/,/^nginx/i AAAAAA\u0026#39; passwd # 匹配到的每一行前追加 $ sed -i \u0026#39;/root/,/nginx/r ./list\u0026#39; passwd $ sed -n \u0026#39;/\\/bin\\/bash/w /tmp/login\u0026#39; passwd 显示行号 命令 含义 = 显示行号 $ sed \u0026#39;/\\/bin\\/bash/=\u0026#39; passwd 修改 命令 含义 s/旧/新/ 更改每行第一个 s/旧/新/3 更改每行第一个 s/旧/新/3 更改每行所有 s/旧/新/2g 更改每行第 2 开始所有 s/旧/新/ig 更改每行所有, 忽略大小写 $ sed -i \u0026#39;s/HADOOP/hadoop/\u0026#39; file $ sed -i \u0026#39;s/HADOOP/hadoop/2\u0026#39; file $ sed -i \u0026#39;s/HADOOP/hadoop/g\u0026#39; file $ sed -i \u0026#39;s/HADOOP/hadoop/2g\u0026#39; file $ sed -i \u0026#39;s/HADOOP/hadoop/ig\u0026#39; file 反向引用 \u0026amp;与\\1 , \\1需要与()一起使用(部分引用时)\n$ sed -i \u0026#39;s/had..../\u0026amp;s/g\u0026#39; file $ sed -i \u0026#39;s/had..../\\1s/g\u0026#39; file $ sed -i \u0026#39;s/\\(had=)..../\\1DOOP/g\u0026#39; file 变量引用 用双引号 用单引号变量 $ sed -i \u0026#34;s/$str1/str2/g\u0026#34; file $ sed -i \u0026#39;s/\u0026#39;$str1\u0026#39;/\u0026#39;$str2\u0026#39;/g\u0026#39; file 脚本练习: 查询 mysql 配置文件 知识点:\n利用sed进行文件查询\n预期:\nmysqld\u0026#39;s num: 22 client\u0026#39;s num: 1 mysql\u0026#39;s num: 1 server\u0026#39;s num: 6 思路:\n获取所有段 根据获取的段信息, 获取每一段下的配置项数目 打印结果 #!/bin/bash # FILENAME=\u0026#34;/root/sed/my.ini\u0026#34; # 1. 获取所有段 function get_segments { item=`sed -n \u0026#39;/\\[.*\\]/p\u0026#39; $FILENAME | sed \u0026#39;s/\\[//g\u0026#39; | sed \u0026#39;s/\\]//g\u0026#39;` echo $item } # 2. 根据获取的段信息, 获取每一段下的配置项数目 function count_segment_num { # 根据 sed 的范围匹配 # 根据参数提供段范围匹配 | 去除 [ 开头 | 去除 # 开头 | 去除空行 items=`sed -n \u0026#39;/\\[\u0026#39;$1\u0026#39;\\]/,/\\[.*\\]/p\u0026#39; $FILENAME | grep -v \u0026#34;^\\[\u0026#34; | grep -v \u0026#34;^#\u0026#34; | grep -v \u0026#34;^$\u0026#34;` # 统计所有项 index=0 for item in $items do index=`expr $index + 1` done echo $index } for seg in `get_segments` do item_count=`count_segment_num $seg` echo \u0026#34;${seg}\u0026#39;s num: $item_count\u0026#34; done sed 文本练习 文本删除: 删除空行和注释行\n$ sed -i \u0026#39;/^#/d\u0026#39; nginx.conf # 删除开头 # $ sed -i \u0026#39;/^$/d\u0026#39; nginx.conf # 删除空行 $ sed -i \u0026#39;/[:blank]*#/d\u0026#39; nginx.conf # 删除 # 前有空格 # 合并写法 $ sed -i \u0026#39;/[:blank]*#/d;/^$/d\u0026#39; nginx.conf 对以非 # 开头行加 *\n# 非 # : [^#] $ sed -i \u0026#39;s/^[^#]/\\*\u0026amp;/g\u0026#39; nginx.conf 文本查找 匹配到以 root 开始的行, 把 login 改为 LOGIN\nsed -i \u0026#39;/^root/s/login/LOGIN/\u0026#39; passwd 修改以 root 开始的行, 到包含 mail 的行, 修改 bin 为 BIN\nsed -i \u0026#39;/^root/,/mail/s/bin/BIN/g\u0026#39; passwd 把文本中数字删除\nsed -i \u0026#39;s/[0-9]*//g\u0026#39; file 文本追加 根据行号\n$ sed -i \u0026#39;3,7/a Append\u0026#39; passwd 匹配到 /bin/bash 的行, 气候追加 After\n$ sed -i \u0026#39;/\\/bin\\/bash/a After\u0026#39; passwd 以 nginx 开头的行前, 添加 Before\n$ sed -i \u0026#39;/^nginx/i Before\u0026#39; passwd 每一行前加 Before\n$ sed -i \u0026#39;i Before\u0026#39; passwd 把 /etc/fstab 内容追加到含 /bin/bash 的行后\n$ sed -i \u0026#39;/\\/bin\\/bash/r /etc/fstab\u0026#39; passwd **把 /bin/bash 的行, 写入 /tmp/sed.txt **\n$ sed -i \u0026#39;/\\bin\\/bash/w /tmp/sed.txt\u0026#39; passwd 把第 10 行到以 nginx 开头的行写入 /tmp/sed.txt\n$ sed -i \u0026#39;10,/\\bin\\/bash/w /tmp/sed.txt\u0026#39; passwd ","date":"2023-12-17T15:16:34+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/文本处理三剑客之sed/cover.webp","permalink":"/p/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/","title":"文本处理三剑客之sed"},{"content":"awk 工作模式 与 sed 相同, 都是逐行处理\n语法格式 awk \u0026lsquo;BEGIN{}pattern{commands}END{} filename\u0026rsquo; stdout | awk \u0026lsquo;BEGIN{}pattern{commands}END{}\u0026rsquo; 语法格式 说明 BEGIN{} 处理文本前执行 pattern 匹配模式 {commands} 处理命令, ;隔开 END{} 处理文本后执行 BEGIN{}, pattern, END{} 多可省略\n内置变量 内置变量 含义 $0 整行内容 $1-$n 按分隔符的第 1-n 个字段 NF (Number Field) 当前行的字段个数(多少列) NR (Number Row) 当前行行号, 从 1 开始计数 FNR (File Number Row) 多文件处理时, 每个文件单独技术, 从 1 开始 FS (Field Separate) 输入字段分隔符, 不指定为空格或 tab RS (Row Separator) 输入行分隔符, 默认回车 OFS (Output Field Separator) 输出字段分隔符 ORS (Output Row Separator) 输出行分隔符, 默认回车 FILENAME 当前输入文件名 ARGC 命令行参数个数 ARGV 命令行参数数组 基本使用:\n$ awk \u0026#39;{print $0}\u0026#39; passwd $ awk \u0026#39;{print $1,$3}\u0026#39; list $ awk \u0026#39;{print NF}\u0026#39; list $ awk \u0026#39;{print NR}\u0026#39; list $ awk \u0026#39;{print FNR}\u0026#39; list awk.txt $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{print $1}\u0026#39; passwd $ awk \u0026#39;BEGIN{RS=\u0026#34;--\u0026#34;}{print $0}\u0026#39; list $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;;OFS=\u0026#34;|\u0026#34;}{print $1,$3}\u0026#39; passwd # 每行输出字段 $ awk \u0026#39;BEGIN{ORS=\u0026#34;--\u0026#34;}{print $0}\u0026#39; passwd $ awk \u0026#39;{print FILENAME}\u0026#39; passwd $ awk \u0026#39;{print ARGC}\u0026#39; passwd list # 3 个参数 awk, passwd, list 格式化输出 printf 格式符 含义 %s 字符串 %d 十进制 %f 浮点数 %x 十六进制 %o 八进制 %e 科学计数法 %c 单个字符 修饰符:\n修饰符 含义 - 左对齐 + 右对齐 # 打印 十六进制与八进制时使用, 在前打印进制标识 示例\n# %s: 默认左对齐 # %10s: 默认右对齐 $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{printf \u0026#34;%s\u0026#34;,$7}\u0026#39; passwd $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{printf \u0026#34;%10s\u0026#34;,$7}\u0026#39; passwd $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{printf \u0026#34;%-10s\u0026#34;,$7}\u0026#39; passwd $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{printf \u0026#34;%d\u0026#34;,$3}\u0026#39; passwd $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{printf \u0026#34;%0.3f\u0026#34;,$3}\u0026#39; passwd $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{printf \u0026#34;%x\u0026#34;,$3}\u0026#39; passwd $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{printf \u0026#34;%#x\u0026#34;,$3}\u0026#39; passwd # 显示 16 进制标识 $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{printf \u0026#34;%0\u0026#34;,$3}\u0026#39; passwd $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}{printf \u0026#34;%e\u0026#34;,$3}\u0026#39; passwd 模式匹配 正则表达式 (固定写法//) 关系运算匹配 正则表达式:****\n# 含有 root 的行 $ awk \u0026#39;BEGIN{FS=\u0026#39;:\u0026#39;}/root/{print $0}\u0026#39; passwd # 以 nginx 开头 $ awk \u0026#39;BEGIN{FS=\u0026#39;:\u0026#39;}/^nginx/{print $0}\u0026#39; passwd 关系运算符:\n关系运算符 含义 \u0026lt; 数值 小于 \u0026gt; 数值 大于 \u0026lt;= 数值 小于等于 \u0026gt;= 数值 大于等于 == 等于 != 不等于 ~ 匹配正则 !~ 不匹配正则 # 第 3 个字段小于 50 $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}$3\u0026lt;50{print $0}\u0026#39; passwd # 第 7 个字段为 /bin/bash $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}$7==\u0026#34;/bin/bash\u0026#34;{print $0}\u0026#39; passwd $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}$7!=\u0026#34;/bin/bash\u0026#34;{print $0}\u0026#39; passwd # 第 3 个字段包含 3 个及以上数字 $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}$3~/[0-9]{3,}/{print $0}\u0026#39; passwd 逻辑运算符:\n逻辑运算符 含义 || 或 \u0026amp;\u0026amp; 与 ! 非 $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}$1==\u0026#34;root\u0026#34;||$1==\u0026#34;nginx\u0026#34;{print $0}\u0026#39; passwd $ awk \u0026#39;BEGIN{FS=\u0026#34;:\u0026#34;}$3\u0026lt;50 \u0026amp;\u0026amp; $3 \u0026gt;30 {print $0}\u0026#39; passwd awk 算数运算 运算符 含义 + 加 - 减 * 乘 / 除 % 取余 ^ 或 ** 乘方 x++ ; x\u0026ndash; 先返回 x, 后 +/- x ++x ; \u0026ndash;x 先 +/- x, 后 返回 x 练习计算课程平均值\n# 右对齐 $ awk \u0026#39;BEGIN{printf \u0026#34;%10s%10s%10s%10s%10s\\n\u0026#34;,\u0026#34;Name\u0026#34;, \u0026#34;YuWen\u0026#34;,\u0026#34;ShuXue\u0026#34;, \u0026#34;English\u0026#34;, \u0026#34;AVG\u0026#34;}{total=$1+$2+$3;AVG=total/3;printf \u0026#34;%10s%10d%10d%10d%10.2f\\n\u0026#34;,$1,$2,$3,$4,AVG}\u0026#39; list $ awk \u0026#39;BEGIN{printf \u0026#34;%-10s%-10s%-10s%-10s%-10s\\n\u0026#34;,\u0026#34;Name\u0026#34;, \u0026#34;YuWen\u0026#34;,\u0026#34;ShuXue\u0026#34;, \u0026#34;English\u0026#34;, \u0026#34;AVG\u0026#34;}{total=$1+$2+$3;AVG=total/3;printf \u0026#34;%-10s%-10d%-10d%-10d%-10.2f\\n\u0026#34;,$1,$2,$3,$4,AVG}\u0026#39; list 条件语句 if-else\n示例: script.awk\nBEGIN{ FS=\u0026#34;:\u0026#34; } { if($3\u0026lt;50) { printf \u0026#34;%-20s%-10s%10d\\n\u0026#34;,\u0026#34;UID\u0026lt;50\u0026#34;,$1,$3 } else if($3\u0026gt;50 \u0026amp;\u0026amp; $3 \u0026lt;100) { printf \u0026#34;%-20s%-10s%10d\\n\u0026#34;,\u0026#34;50\u0026lt;UID\u0026lt;100\u0026#34;,$1,$3 } else { printf \u0026#34;%-20s%-10s%10d\\n\u0026#34;,\u0026#34;UID\u0026gt;100\u0026#34;,$1,$3 } } $ awk -f script.awk /etc/passwd 循环语句 do-while while for 计算 1+2+\u0026hellip;100\ndo-while BEGIN{ do{ sum += i i++ }while(i\u0026lt;=100) print sum } while BEGIN{ while(i\u0026lt;=100) { sum += i i++ } print sum } for BEGIN{ for(i=0;i\u0026lt;=100;i++) { sum += i } print sum } 练习: 打印平均分大于 70的, 并计算平均分\nBEGIN{ printf \u0026#34;%-10s%-10s%-10s%-10s%-10s\\n\u0026#34;,\u0026#34;Name\u0026#34;,\u0026#34;YuWen\u0026#34;,\u0026#34;Math\u0026#34;,\u0026#34;English\u0026#34;,\u0026#34;AVG\u0026#34; } { total = $2 + $3 + $3 avg = total / 3 if (avg \u0026gt; 70) { printf \u0026#34;%-10s%-10d%-10d%-10d%-0.2f\\n\u0026#34;,$1,$2,$3,$4,avg score_yuwen += $2 score_math += $3 score_english += $4 score_avg += avg count++ } } END{ printf \u0026#34;%-10s%-10.2f%-10.2f%-10.2f%-0.2f\\n\u0026#34;,\u0026#34;\u0026#34;,score_yuwen/count,score_math/count,score_english/count,score_avg/count } 字符串函数 函数名 解释 返回值 length(str) 计算字符串长度 长度值 index(str1,str2) 在 str1 中查找 str2 返回位置索引, 从 1 计数 tolower(str) 转小写 转小写后的字符串 toupper(str) 转大写 转大写后的字符串 substr(str,m,n) 从 str m 字符, 截取 n 位(n 可省略) 截取后的子串 split(str,arr,fs) 按 fs 切割字符串, 结果保存到 arr 切割后的子串个数 match(str,RE) 与 index() 类似, 但支持正则(RE) 返回索引位置 sub(RE,RepStr,str) 在 str 中搜索符合 RE 的子串将其替换为 RepStr; 只替换第一个 替换个数 gsub(RE,RepStr,str) 在 str 中搜索符合 RE 的子串将其替换为 RepStr; 替换全部 替换个数 1. 打印 passwd 每个字段长度:\nBEGIN{ FS=\u0026#34;:\u0026#34; } { i=1 while(i\u0026lt;=NF) { if(i==NF) printf \u0026#34;%d\u0026#34;,length($i) else printf \u0026#34;%d:\u0026#34;,length($i) i++ } print \u0026#34;\u0026#34; } 2. 查询\u0026quot;I have a dream\u0026quot;中\u0026quot;ea\u0026quot;索引\n$ awk \u0026#39;BEGIN{str=\u0026#34;I have a dream\u0026#34;;localtion=index(str,\u0026#34;ea\u0026#34;);print localtion}\u0026#39; # 12 $ awk \u0026#39;BEGIN{str=\u0026#34;I have a dream\u0026#34;;localtion=match(str,\u0026#34;ea\u0026#34;);print localtion}\u0026#39; # 12 3. 大小写转换\n$ awk \u0026#39;BEGIN{str=\u0026#34;I have a dream\u0026#34;;print tolower(str)}\u0026#39; # i have a dream $ awk \u0026#39;BEGIN{str=\u0026#34;I have a dream\u0026#34;;print toupper(str)}\u0026#39; # I HAVE A DREAM 4. 切分数组\n$ awk \u0026#39;BEGIN{str=\u0026#34;I have a dream\u0026#34;;split(str,arr,\u0026#34; \u0026#34;);print arr[2]}\u0026#39; $ awk \u0026#39;BEGIN{str=\u0026#34;I have a dream\u0026#34;;split(str,arr);print arr[2]}\u0026#39; # 默认空格分隔 # have # 遍历, 不是顺序遍历 $ awk \u0026#39;BEGIN{str=\u0026#34;I have a dream\u0026#34;;split(str,arr);for(a in arr) print arr[a]}\u0026#39; # dream # I # have # a 5. 搜索第一个出现的数字\n# 正则必须用 // $ awk \u0026#39;BEGIN{str=\u0026#34;I have a 123 dream\u0026#34;; print match(str, /[0-9]/)}\u0026#39; # 10 6. 截取子串\n$ awk \u0026#39;BEGIN{str=\u0026#34;I have a 123 dream\u0026#34;; print substr(str,3,7)}\u0026#39; # have a $ awk \u0026#39;BEGIN{str=\u0026#34;I have a 123 dream\u0026#34;; print substr(str,3)}\u0026#39; # have a 123 dream 7. 替换数字\n$ awk \u0026#39;BEGIN{str=\u0026#34;I have a 123 dream 324 hello\u0026#34;; print sub(/[0-9]+/,\u0026#34;$\u0026#34;,str); print str}\u0026#39; # 1 # I have a $ dream 324 hello $ awk \u0026#39;BEGIN{str=\u0026#34;I have a 123 dream 324 hello\u0026#34;; print gsub(/[0-9]+/,\u0026#34;$\u0026#34;,str); print str}\u0026#39; # 2 # I have a $ dream $ hello awk 常用选项 选项 说明 -v 参数传递 -f 指定脚本文件 -F 指定分隔符 -V 查看版本 如果变量有空格, 要使用\u0026quot;\u0026quot;\n$ num=13 $ var=\u0026#34;hello world\u0026#34; $ awk -v num1=$num -v var1=$var \u0026#39;BEGIN{print num1,var1}\u0026#39; # awk: fatal: cannot open file `BEGIN{print num1,var1}\u0026#39; for reading (No such file or directory) $ awk -v num1=\u0026#34;$num\u0026#34; -v var1=\u0026#34;$var\u0026#34; \u0026#39;BEGIN{print num1,var1}\u0026#39; # 13 hello world $ awk -F \u0026#34;:\u0026#34; \u0026#39;{print $0}\u0026#39; /etc/passwd awk 与 shell 中数组 shell 中数组 下标从 0 开始\n打印数组:\n$ arr=(\u0026#34;kubernetes\u0026#34; \u0026#34;etcd\u0026#34; \u0026#34;time\u0026#34; \u0026#34;redis\u0026#34;) # 打印数组 $ echo ${arr[@]} $ echo ${arr[*]} # kubernetes etcd time redis # 打印元素 $ echo ${arr[2]} # time 打印数组/元素长度; 分片访问; 元素操作; 删除元素:\n$ arr=(\u0026#34;kubernetes\u0026#34; \u0026#34;etcd\u0026#34; \u0026#34;time\u0026#34; \u0026#34;redis\u0026#34;) # 打印数组 $ echo ${#arr[@]} $ echo ${#arr[*]} # 4 # 打印元素 $ echo ${#arr[3]} # 5 # 分片访问 $ echo ${arr[@]:1:3} # etcd time redis # 元素赋值 $ arr[2]=mysqlserver $ echo ${arr[@]} # kubernetes etcd mysqlserver redis # 元素内容替换 $ echo ${arr[@]/e/E} # kubErnetes Etcd mysqlsErver rEdis $ echo ${arr[@]//e/E} # kubErnEtEs Etcd mysqlsErvEr rEdis # 元素删除 *** 通过下标删除后, 被删除的下标的元素为空, 原数组的其他元素下标不变 $ unset arr[0] $ echo ${arr[@]} # etcd mysqlserver redis $ unset arr[0] $ echo ${arr[@]} # etcd mysqlserver redis $ unset arr[1] $ echo ${arr[@]} # mysqlserver redis # 删除数组 $ unset arr 通过下标删除后, 被删除的下标的元素为空, 原数组的其他元素下标不变\n数组的遍历\n$ arr=(\u0026#34;kubernetes\u0026#34; \u0026#34;etcd\u0026#34; \u0026#34;time\u0026#34; \u0026#34;redis\u0026#34;) $ for a in ${arr[@]}; do echo $a; done # kubernetes # etcd # time # redis awk 中数组 脚本练习 数据生成脚本:\n#!/bin/bash # function create_random() { min=$1 max=$(($2-$min+1)) num=$(date +%s%N) echo $(($num%$max+min)) } INDEX=1 while true do for user in allen mike jerry tracy han lilei do COUNT=$RANDOM NUM1=`create_random 1 $COUNT` NUM2=`expr $COUNT - $NUM1` echo \u0026#34;`date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;` $INDEX Batches: user $user insert $COUNT records into database:product table:detail, insert $NUM1 records successfully, failed $NUM2 records\u0026#34; \u0026gt;\u0026gt; ./db.log.`date +%Y%m%d` INDEX=`expr $INDEX + 1` done done 数据格式\n2023-12-12 02:49:31 1 Batches: user allen insert 25719 records into database:product table:detail, insert 24482 records successfully, failed 1237 records 2023-12-12 02:49:31 2 Batches: user mike insert 32653 records into database:product table:detail, insert 26055 records successfully, failed 6598 records 2023-12-12 02:49:31 3 Batches: user jerry insert 16986 records into database:product table:detail, insert 11636 records successfully, failed 5350 records 2023-12-12 02:49:31 4 Batches: user tracy insert 31899 records into database:product table:detail, insert 9250 records successfully, failed 22649 records 2023-12-12 02:49:31 5 Batches: user han insert 24256 records into database:product table:detail, insert 24033 records successfully, failed 223 records 统计所有成功, 失败, 总共记录数\ncount.awk:\nBEGIN{ printf \u0026#34;%-10s%-20s%-20s%-20s\\n\u0026#34;,\u0026#34;User\u0026#34;,\u0026#34;Total\u0026#34;,\u0026#34;Sucess\u0026#34;,\u0026#34;Failed\u0026#34; } { TOTAL[$6]+=$8 SUCESS[$6]+=$14 FAILED[$6]+=$18 } END{ for(t in TOTAL) { total += TOTAL[t] sucess += SUCESS[t] failed += FAILED[t] printf \u0026#34;%-10s%-20s%-20s%-20s\\n\u0026#34;,t,TOTAL[t],SUCESS[t],FAILED[t] } printf \u0026#34;%-10s%-20s%-20s%-20s\\n\u0026#34;,\u0026#34;\u0026#34;,total,sucess,failed } $ awk -f count.awk db.log.20231212 User Total Sucess Failed tracy 6096344 2963340 3133004 allen 6293470 3182865 3110605 mike 5845083 2912982 2932101 jerry 5996178 3080723 2915455 lilei 6217104 3028971 3188133 han 5923975 3089899 2834076 36372154 18258780 18113374 2. 打印丢失记录的行数\n一条记录行中, 总记录数 != 成功记录数 + 失败记录数\n$ awk \u0026#39;{if($8 != $14 + $18) print NR}\u0026#39; db.log.20231212 ","date":"2023-12-17T15:16:07+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/文本处理三剑客之awk/cover.jpg","permalink":"/p/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bawk/","title":"文本处理三剑客之awk"},{"content":"函数功能划分: 函数定义 函数功能 function get_all_group 获取所有进程组 function get_all_process 获取所有进程 function get_process_info 返回进程详细信息(运行状态, PID, CPU, MEM, 启动时间) function get_all_process_by_group 返回进程组中所有进程名称 函数实现: function get_all_group: 根据对应字段获取其下的所有组名\nHOME_DIR=\u0026#34;/root/shell\u0026#34; CONFIG_FILE=\u0026#34;process.cfg\u0026#34; function get_all_group { if [ ! -e $HOME_DIR/$CONFIG_FILE ]; then echo \u0026#34;$CONFIG_FIEL is not exist. Please check...\u0026#34; exit 1 else G_LIST=`sed -n \u0026#39;/\\[GROUP_LIST\\]/,/\\[.*\\]/p\u0026#39; process.cfg | egrep -v \u0026#34;(\\[.*\\]|^$)\u0026#34;` echo \u0026#34;$G_LIST\u0026#34; fi } function get_all_process 遍历所有组, 获取其下的所有进程\nfunction get_all_processes { for g in `get_all_group`;do P_LIST=`sed -n \u0026#39;/\\[\u0026#39;$g\u0026#39;\\]/,/\\[.*\\]/p\u0026#39; process.cfg | egrep -v \u0026#34;^\\[|^$\u0026#34;` echo $P_LIST done } function get_process_info 继续拆分函数, 拆分为 get_process_pid_by_name, get_process_info_by_pid 可直接使用 ps aux 把统计的名称的数据一并处理 get_process_pid_by_name:\nfunction get_process_pid_by_name { # 只接受一个参数 if [ $# != 1 ]; then return 1 # 非 0 即为错误 else # 过滤掉脚本本身 pids=`ps -ef | grep \u0026#34;$1\u0026#34; | grep -v grep | grep -v $0 | awk \u0026#39;{print $2}\u0026#39;` echo $pids fi } get_process_info_by_pid:\nfunction get_process_info_by_pid { if [ `ps -ef | awk -v p_id=$1 \u0026#39;$2==p_id{print}\u0026#39; | wc -l` == 1 ]; then pro_status=\u0026#34;RUNNING\u0026#34; else pro_status=\u0026#34;STOPED\u0026#34; fi pro_cpu=`ps aux | awk -v p_id=$1 \u0026#39;$2==p_id{print $3}\u0026#39;` pro_mem=`ps aux | awk -v p_id=$1 \u0026#39;$2==p_id{print $4}\u0026#39;` pro_start=\u0026#34;`ps -p $1 -o lstart`\u0026#34; } function get_all_process_by_group function is_group_in_config { for g in `get_all_group`;do if [ \u0026#34;$1\u0026#34; == $g ];then return 0 fi done return 1 } function get_all_processes_by_group { is_group_in_config $1 if [ $? == 0 ];then p_list=`sed -n \u0026#39;/\\[\u0026#39;$1\u0026#39;\\]/,/\\[.*\\]/p\u0026#39; $HOME_DIR/$CONFIG_FILE | egrep -v \u0026#34;(^$|^#|^\\[)\u0026#34;` echo $p_list else echo \u0026#34;GroupName $1 is not in process.cfg\u0026#34; fi } 主流程设计 接受参数 行为 无参数 打印所有信息 -g 组名 接受多个组名, 打印组下所有进程 进程名 打印指定进程名 if [ $# -gt 0 ];then if [ $1 == \u0026#34;-g\u0026#34; ];then # 接受组名, 把参数左移, 去掉 -g shift for gn in $@; do is_group_in_config $gn || continue for pn in `get_all_processes_by_group`;do is_process_in_config $pn \u0026amp;\u0026amp; format_print $pn $gn done done else for pn in $@;do is_process_in_config $pn \u0026amp;\u0026amp; format_print $pn $gn done fi else # 没有参数, 打印所有 for pn in `get_all_processes`;do gn=`get_group_by_process_name $pn` is_process_in_config $pn \u0026amp;\u0026amp; format_print $pn $gn done fi 完整脚本 #!/bin/bash # # Func: Get app info from process.cfg HOME_DIR=\u0026#34;/root/shell\u0026#34; CONFIG_FILE=\u0026#34;process.cfg\u0026#34; this_pid=$$ function get_all_group { G_LIST=`sed -n \u0026#39;/\\[GROUP_LIST\\]/,/\\[.*\\]/p\u0026#39; process.cfg | egrep -v \u0026#34;(\\[.*\\]|^$)\u0026#34;` echo \u0026#34;$G_LIST\u0026#34; } function get_all_processes { for g in `get_all_group`;do P_LIST=`sed -n \u0026#39;/\\[\u0026#39;$g\u0026#39;\\]/,/\\[.*\\]/p\u0026#39; process.cfg | egrep -v \u0026#34;^\\[|^$\u0026#34;` echo $P_LIST done } function get_process_pid_by_name { # 只接受一个参数 if [ $# != 1 ]; then return 1 # 非 0 即为错误 else pids=`ps -ef | grep \u0026#34;$1\u0026#34; | grep -v grep | grep -v $0 | awk \u0026#39;{print $2}\u0026#39;` echo $pids fi } function get_process_info_by_pid { if [ `ps -ef | awk -v p_id=$1 \u0026#39;$2==p_id{print}\u0026#39; | wc -l` == 1 ]; then pro_status=\u0026#34;RUNNING\u0026#34; else pro_status=\u0026#34;STOPED\u0026#34; fi pro_cpu=`ps aux | awk -v p_id=$1 \u0026#39;$2==p_id{print $3}\u0026#39;` pro_mem=`ps aux | awk -v p_id=$1 \u0026#39;$2==p_id{print $4}\u0026#39;` pro_start_time=\u0026#34;`ps -p $1 -o lstart`\u0026#34; } function is_group_in_config { for g in `get_all_group`;do if [ \u0026#34;$1\u0026#34; == $g ];then return 0 fi done echo \u0026#34;Group $1 is not in process.cfg\u0026#34; return 1 } function get_all_processes_by_group { is_group_in_config $1 if [ $? == 0 ];then p_list=`sed -n \u0026#39;/\\[\u0026#39;$1\u0026#39;\\]/,/\\[.*\\]/p\u0026#39; $HOME_DIR/$CONFIG_FILE | egrep -v \u0026#34;(^$|^#|^\\[)\u0026#34;` echo $p_list else echo \u0026#34;GroupName $1 is not in process.cfg\u0026#34; fi } function format_print { ps -ef | grep $1 | grep -v grep | grep -v $this_pid \u0026amp;\u0026gt; /dev/null if [ $? -eq 0 ]; then pids=`get_process_pid_by_name $1` for pid in $pids; do get_process_info_by_pid $pid awk -v p_name=$1 -v g_name=$2 -v p_id=$pid -v p_status=$pro_status -v p_cpu=$pro_cpu -v p_mem=$pro_mem $p_start_time=\u0026#34;$pro_start_time\u0026#34; \u0026#39;BEGIN{printf \u0026#34;%-10s%-10s%-5s%-5s%-5s%-5s%-15s\u0026#34;,p_name,g_name,p_id,p_status,p_cpu,p_mem,p_start_time}\u0026#39; done else # 进程不存在 awk -v p_name=$1 -v g_name=$2 \u0026#39;BEGIN{printf \u0026#34;%-10s%-10s%-5s%-5s%-5s%-5s%-15s\u0026#34;,p_name,g_name,\u0026#34;NULL\u0026#34;,\u0026#34;STOPED\u0026#34;,\u0026#34;NULL\u0026#34;,\u0026#34;NULL\u0026#34;,\u0026#34;NULL\u0026#34;}\u0026#39; fi } function is_process_in_config { for pn in `get_all_process`;do if [ $pn == $1 ];then return fi done echo \u0026#34;Process $1 is not in process.cfg\u0026#34; return 1 } function get_group_by_process_name { for gn in `get_all_group`;do for pn in `get_all_process_by_group $gn`;do if [ $pn == $1 ]; then ehco $gn fi done done } if [ $# -gt 0 ];then if [ $1 == \u0026#34;-g\u0026#34; ];then # 接受组名, 把参数左移, 去掉 -g shift for gn in $@; do is_group_in_config $gn || continue for pn in `get_all_processes_by_group`;do is_process_in_config $pn \u0026amp;\u0026amp; format_print $pn $gn done done else for pn in $@;do is_process_in_config $pn \u0026amp;\u0026amp; format_print $pn $gn done fi else # 没有参数, 打印所有 for pn in `get_all_processes`;do gn=`get_group_by_process_name $pn` is_process_in_config $pn \u0026amp;\u0026amp; format_print $pn $gn done fi ","date":"2023-12-17T15:15:36+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/Shell脚本之进程管理/cover.jpg","permalink":"/p/shell%E8%84%9A%E6%9C%AC%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/","title":"Shell脚本之进程管理"},{"content":"mysql 命令参数 参数 含义 -u 用户 -p 密码 -h 要连接的主机 -D 指定数据库 -e 要执行的 sql 语句 -B 输出以 tab 分隔 -N 不显示列头 -E 垂直显示 -H 以 html 形式输出 -X xml 格式输出 shell 脚本接受 与, 操作 mysql\n#!/bin/bash # user=\u0026#34;dbuser\u0026#34; password=\u0026#34;123456\u0026#34; host=\u0026#34;127.0.0.1\u0026#34; db_name=\u0026#34;$1\u0026#34; # 参数中用空格时要用双引号 SQL=\u0026#34;$2\u0026#34; mysql -u\u0026#34;$user\u0026#34; -p\u0026#34;$password\u0026#34; -h\u0026#34;$host\u0026#34; -D\u0026#34;db_name\u0026#34; -B -e \u0026#34;$SQL\u0026#34; $ sh operator.sh school \u0026#34;insert into score(\u0026#34;231\u0026#34;,\u0026#34;1234\u0026#34;,100)\u0026#34; 使用 shell 导入数据\n#/bin/bash # user=\u0026#34;dbuser\u0026#34; password=\u0026#34;123456\u0026#34; host=\u0026#34;127.0.0.1\u0026#34; IFS=\u0026#34;|\u0026#34; # shell 默认以空格与 tab 切分, 通过 IFS 改变默认分隔符 cat data.txt | while read id name birth sex do if [ $id -gt 2023 ];then mysql -u\u0026#34;$user\u0026#34; -p\u0026#34;$password\u0026#34; -h\u0026#34;$host\u0026#34; -e \u0026#34;Insert into school.student values(\u0026#39;$id\u0026#39;,\u0026#39;$name\u0026#39;,\u0026#39;$birth\u0026#39;,\u0026#39;$sex\u0026#39;)\u0026#34; fi done 利用 mysqldump 进行数据库备份 mysqldump 参数 含义 -u -p -h -d \u0026ndash;no-data 只导出表结构 -t \u0026ndash;no-create-info 只导出数据, 不导出建表语句 -A \u0026ndash;all-databases \u0026ndash;all-databases -B \u0026ndash;databases 导出一个或多个数据库 备份 school 下的 score 表, 通过 ftp 上传到目标服务器\n#!/bin/bash # db_user=\u0026#34;dbuser\u0026#34; db_password=\u0026#34;123456\u0026#34; db_host=\u0026#34;127.0.0.1\u0026#34; ftp_user=\u0026#34;fpt_user\u0026#34; ftp_password=\u0026#34;123\u0026#34; ftp_host=\u0026#34;127.0.0.1\u0026#34; src_dir=\u0026#34;/data/bak\u0026#34; dst_dir=\u0026#34;/data/backup\u0026#34; time=\u0026#34;`date +%Y%m%d%H%M%S`\u0026#34; filename=\u0026#34;school_score_${time}.bak\u0026#34; function auto_ftp { ftp -inv \u0026lt;\u0026lt; EOF open $ftp_host user $ftp_user $ftp_password cd $dst_dir put $1 bye EOF } # 备份 school 数据库下的 score 表 # \u0026amp;\u0026amp; 只有前一天命令正确执行在执行后面命令 mysqldump -u\u0026#34;$db_user\u0026#34; -p\u0026#34;$db_password\u0026#34; -h\u0026#34;db_host\u0026#34; school score \u0026gt; $src_dir/$filename \u0026amp;\u0026amp; auto_ftp $src_dir/$filename ","date":"2023-12-17T15:14:43+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/Shell脚本与Mysql交互/cover.png","permalink":"/p/shell%E8%84%9A%E6%9C%AC%E4%B8%8Emysql%E4%BA%A4%E4%BA%92/","title":"Shell脚本与Mysql交互"},{"content":"函数定义与使用 语法格式\nnginx 守护进程\n#!/bin/bash # while true do ps -ef | grep nginx | grep -v grep | grep -v $$ \u0026amp;\u0026gt; /dev/null if [ $? -eq 0 ]; then echo \u0026#34;nginx is running\u0026#34; sleep 3s else echo \u0026#34;nginx is stop, starting\u0026#34; systemctl start nginx fi done 简单计算函数\n#!/bin/bash # function calcu { case $2 in +) echo `expr $1 + $3` ;; -) echo `expr $1 - $3` ;; \\*) echo `expr $1 \\* $3` ;; /) echo `expr $1 / $3` ;; esac } calcu $1 $2 $3 函数返回值 return echo return 返回值案例\n#!/bin/bash # function is_nginx_isrunning { ps -ef | grep nginx | grep -v grep | grep $$ \u0026amp;\u0026gt; /dev/null if [ $? -eq 0 ]; then return else return 1 fi } is_nginx_running \u0026amp;\u0026amp; echo \u0026#34;nginx is running\u0026#34; || echo \u0026#34;nginx is not running\u0026#34; echo 返回值案例\n#!/bin/bahs # function get_users { users=`cat /etc/passwd | cut -d \u0026#34;:\u0026#34; -f1` echo $users } userlist=`get_users` index=1 for u in ${userlist[@]} # 也可以 $userlist do echo \u0026#34;This is $index user: $u\u0026#34; index=$(($index+1)) done 全局变量与局部变量 Shell 中默认为全局变量\n局部变量使用 local 关键字\n#!/bin/bash # var1=1 function set_var2 { var2=2 } echo $var1 echo $var2 set_var2 echo $var2 function set_var3 { local var3=3 } set_var3 echo $var3 输出:\n1 2 函数库 库文件通常以 .lib 结尾\nbase_function.lib\n#!/bin/echo # 提示不可运行 function add { echo \u0026#34;`expr $1 + $2`\u0026#34; } function divide { echo \u0026#34;`expr $1 / $2`\u0026#34; } function sys_load { echo \u0026#34;Memory Info\u0026#34; echo free -m echo echo \u0026#34;Disk Info\u0026#34; echo df -h echo } use_base_function.sh\n#!/bin/bash # . /root/shell/base_function.lib # 也可以用 source; 要用绝对路径 add 1 2 divide 7 3 sys_load ","date":"2023-12-17T15:14:06+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/Shell函数/cover.png","permalink":"/p/shell%E5%87%BD%E6%95%B0/","title":"Shell函数"},{"content":"常用选项 (找文件) 语法格式 选项参数表: 常用 解释 -name find /etc -name \u0026lsquo;*conf\u0026rsquo; -iname find /etc -name \u0026lsquo;*conf\u0026rsquo; 忽略大小写 -user find . -user username -group find . -group groupname -type find . -type f -size find /etc -size -10000c 查找小于10000字节的文件 -type 参数 -type 参数 对应文件类型 f 文件类型 d 目录 c 字符设备文件 b 块设备文件 l 链接文件 p 管道文件 -size 参数 -size 参数 说明 -n find /etc -size -10000c 查找小于10000字节的文件 +n find /etc -size +1M 查找大于10000字节的文件 c : 字节\nk: 1000字节\nM : Mb\n-mtime 以天为单位\n-mtime 参数 说明 -n n 天内修改的文件 +n n 天以前修改的文件 n 第 n 天修改的文件 示例\n查找 /etc 下 5 天内修改的文件, 并以 conf 结尾 查找 /etc 10 天前修改的文件, 属主为 root $ find /etc -mtime -5 -name \u0026#39;*conf\u0026#39; $ find /etc -mtime +10 -user root -mmin -mmin 参数 说明 -n n 分钟内修改 +n n 分钟以前修改 -mindepth 要作为 find 第一个选项\n从第 n 级子目录开始搜索\n要搜索的目录为第一个目录\n-maxdepth -newer\n$ find /etc -newer 123.txt # 查找比 123.txt 更新的文件 find 操作 (执行) -print -exec -ok : 与 -exec 一致, 需要用户交互 -exec 删除\n$ find ./etc -name \u0026#34;*conf\u0026#34; -exec rm -rf {} \\; 复制\n$ find ./etc -size +1M -exec cp {} ./test/ \\; 练习: 把 /var/log/ 下 7 天以上的文件删除\n$ find /var/log/ -name \u0026#34;*.log\u0026#34; -mtime +7 -exec rm -rf {} \\; 逻辑运算符 -a 与 -o 或 -not | ! 非 $ find . -not -user \u0026lt;username\u0026gt; $ find . ! -user \u0026lt;username\u0026gt; $ find -type f -a -user \u0026lt;username\u0026gt; -a -size +300c # 属主为 ? 或以 .yml 结尾的文件 $ find . -type f -a \\( -user \u0026lt;username\u0026gt; -o -name \u0026#39;*.yml\u0026#39; \\) 其他查找命令 locate: 默认部分匹配 位于软件包 mlocate\nfind 是直接从磁盘中查找, locate 是从数据库中查找\n数据库文件位置: /var/lib/mlocate/mlocate.db -\u0026gt; 通过 updatedb 更新数据库文件\n数据库配置文件: /etc/updatedb.conf\n在后台 cron 定定时任务执行更新\nwhereis: 查找二进制文件 选项 说明 -b 二进制文件 -m 返回帮助文档 -s 返回源代码文件 which: 查找二进制程序文件 which mysql ","date":"2023-12-17T15:13:03+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/find文件查找/cover.png","permalink":"/p/find%E6%96%87%E4%BB%B6%E6%9F%A5%E6%89%BE/","title":"Find文件查找"},{"content":"1. 变量替换 语法 解释 ${变量#匹配规则} 从头开始匹配, 删除最短匹配 ${变量##匹配规则} 从头开始匹配, 删除最长匹配(贪婪匹配) ${变量%匹配规则} 从尾部开始匹配, 删除最短匹配 ${变量%%匹配规则} 从尾部开始匹配, 删除最长匹配(贪婪匹配) ${变量/旧字符串/新字符串} 替换变量, 只替换第一个匹配项 ${变量//旧字符串/新字符串} 替换变量, 替换所有匹配项 示例: $ var=\u0026#34;I love you, Do you love me\u0026#34; ${变量#匹配规则}\n$ var1=${var#*ov} # 匹配时从第一个字符开始匹配 $ echo $var1 输出:\ne you, Do you love me ${变量##匹配规则}\n$ var2=${var##*ov} $ echo $var2 输出:\ne me ${变量%匹配规则}\n$ var3=${var%ov*} $ echo $var3 输出:\nI love you, Do you l ${变量%%匹配规则}\n$ var4=${var%%ov*} $ echo $var4 输出:\nI l ${变量/旧字符串/新字符串}\n$ echo $PATH 输出:\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin $ var5=${PATH/bin/BIN} $ echo $var5 输出:\n/usr/local/sBIN:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin ${变量//旧字符串/新字符串}\n$ var6=${PATH//bin/BIN} $ echo $var6 输出:\n/usr/local/sBIN:/usr/local/BIN:/usr/sBIN:/usr/BIN:/sBIN:/BIN:/snap/BIN 2. 变量测试 变量测试实际用得较少, 大多都可以通过if-else替换\n变量替换参照表\n示例: var=${str-expr}\n1. 没有设置变量\n$ unset var $ echo ${var-expr} 输出:\nexpr 2. 设置变量, 且变量为空\n$ var= $ echo ${var-expr} 输出:\n# 输出空值 3. 设置变量, 且变量非空\n$ var=test $ echo ${var-expr} 输出:\ntest 3. 字符串处理 计算字符串长度 语法 说明 方法一 ${#string} 无 方法二 expr length \u0026ldquo;$string\u0026rdquo; string 有空格，则必须加双引号, (建议加上 \u0026quot;\u0026quot; ) 示范\n$ str=shellLearning $ echo ${#str} # 13 $ echo $(expr length \u0026#34;$str\u0026#34;) # 13 获取字符在字符串中的索引位置 $ expr index \u0026#34;$string\u0026#34; $substring $ var=\u0026#34;kubernetes is popular\u0026#34; $ idx=`expr index \u0026#34;$var\u0026#34; t` # var 中有空格, 加 \u0026#34;\u0026#34; $ echo $idx # 8 $ idx=`expr index \u0026#34;$var\u0026#34; te` # 子串会被拆成一个个字符一次匹配, 返回第一个匹配 $ echo $idx # 4 获取子串长度 $ expr match \u0026#34;$var\u0026#34; substr # 子串必须从头第一个字符开始匹配 $ var=\u0026#34;kubernetes is popular\u0026#34; $ idx=`expr match \u0026#34;$var\u0026#34; popu` $ echo $idx # 0 $ idx=`expr match \u0026#34;$var\u0026#34; ku.*` $ echo $idx # 21 抽取子串 语法 说明 ${string:position} 从 string 中 position 开始到结尾 ${string:position:length} 从 string 中 position 开始, 匹配长度为 length ${string: -position} 从右边开始匹配 ${string:(position)} 从左边开始匹配 expr substr \u0026ldquo;$string\u0026rdquo; $position $length 从 string 中 position 开始, 匹配长度为 length :索引从 0 开始\nexpr 索引从 1 开始\n$ var=\u0026#34;kubecetes istio etcd operator\u0026#34; $ substr=${var:7} $ echo $substr # es istio etcd operator $ substr=${var:7:6} $ echo $substr # es ist $ substr=${var: -7} $ echo $substr # perator $ substr=${var:(-7)} $ echo $substr # perator $ substr=`expr substr \u0026#34;$var\u0026#34; 7 6` # tes is 脚本练习 思路:\n根据功能划分, 编写响应函数 实现定义函数 主流程设计 #!/bin/bash # str=\u0026#34;Bigdata process framework is Hadoop,Hadoop is an open source project\u0026#34; function print_tips { echo \u0026#34;************************************\u0026#34; echo \u0026#34;(1) 打印 string 长度\u0026#34; echo \u0026#34;(2) 删除字符串中所有的 Hadoop\u0026#34; echo \u0026#34;(3) 替换第一个 Hadoop 为 Mapreduce\u0026#34; echo \u0026#34;(4) 替换全部 Hadoop 为 Mapreduce\u0026#34; echo \u0026#34;************************************\u0026#34; } function len_of_string { echo \u0026#34;${#str}\u0026#34; } function del_hadoop { echo \u0026#34;${str//Hadoop/}\u0026#34; } function replace_hadoop_mapreduce_first { echo \u0026#34;${str/Hadoop/Mapreduce}\u0026#34; } function replace_hadoop_mapreduce_all { echo \u0026#34;${str//Hadoop/Mapreduce}\u0026#34; } while true do echo \u0026#34;[string: $str]\u0026#34; echo print_tips read -p \u0026#34;Please input your choice(1|2|3|4|q|Q): \u0026#34; choice case $choice in 1) len_of_string ;; 2) del_hadoop ;; 3) replace_hadoop_mapreduce_first ;; 4) replace_hadoop_mapreduce_all ;; q|Q) exit ;; *) echo \u0026#34;Error input, only {1|2|3|4|q|Q}\u0026#34; ;; esac done 4. 命令替换 使用 `command` 使用 $(command) 示例 获取系统所有用户\n#!/bin/bash # index=1 for user in `cat /etc/passwd | cut -d \u0026#34;:\u0026#34; -f 1` do echo \u0026#34;This is $index user: $user\u0026#34; index=$(($index+1)) # ((index++)) done 获取年份\n$ echo $(($(date +%Y) + 1)) # 2024 # 等价 $ echo $((num1+num2)) $ echo $(($num1+$num2)) 计算星期\n#!/bin/bash # echo \u0026#34;Today is the $(date +%j) of year.\u0026#34; echo \u0026#34;This year has passed $(($(date +%j) / 7)) weeks\u0026#34; echo \u0026#34;This year has $(((365-$(date +%j)) / 7))\u0026#34; 判断进程是否存活\n#!/bin/bash # nginx_num=$(ps -ef | grep nginx | grep -v grep | wc -l) if [ $nginx_num -eq 0 ]; then systemctl start nginx fi 有类型变量 declare 与 typeset 命令\n两者等价\ndeclare 命令参数表:\n与之相反, 取消变量, 把-改为+\n5. Bash 数学运算 expr 与 $(()) $(()) : 只可用于加减乘除\n参数表\n$ num1=17 $ num2=5 $ expr $num1 + $num2 # 22 $ expr $num1 - $num2 # 12 $ expr $num1 \\* $num2 # 85 $ expr $num1 / $num2 # 3 $ expr $num1 % $num2 # 2 $ expr $num1 \\\u0026gt; $num2 # 真为 1, 假为 0 # 1 $ expr $num1 \\\u0026lt; $num2 # 0 练习:\n#!/bin/bash # while true do read -p \u0026#34;Pls input a integer: \u0026#34;: num expr $num + 1 \u0026amp;\u0026gt; /dev/null # expr 只支持整数 if [ $? -eq 0 ]; then if [ `expr $num \\\u0026gt; 0` -eq 1 ]; then for((i=1;i\u0026lt;$num;i++)) do sum=`expr $sum + $i ` done echo \u0026#34;1+2+...+$num = $sum\u0026#34; exit else echo \u0026#34;error not positive\u0026#34; fi else echo \u0026#34;error inlegle input\u0026#34; fi done bc 只支持指数运算 $ echo \u0026#34;7.6/3.7\u0026#34; | bc # 2 $ echo \u0026#34;scale=4;7.6/3.7\u0026#34; | bc # bc 中 scale 为精度 # 2.0540 ","date":"2023-12-17T14:56:44+08:00","image":"https://blog-source-mkt.oss-cn-chengdu.aliyuncs.com/blog_source/post/images/Shell变量/cover.webp","permalink":"/p/shell%E5%8F%98%E9%87%8F/","title":"Shell变量"}]